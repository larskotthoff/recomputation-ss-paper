\section{Case study \#2: Recomputing parallel and distributed experiments}

The Recomputation.org project has shown that recomputation via a 
VM can be successful when using sequential code on a single
machine.  Our second case study examined 
whether this approach could be extended to cover multi-core
parallel or distributed systems. For parallelism research, the end goal is
typically performance: can using more cores make something run faster, or allow
a larger dataset to be processed in the available time? It is not clear whether
a virtualised environment would affect the quality of the results for these
kinds of experiments. For distributed systems, a single VM would
obviously not suffice, but could multiple VMs be used?  To address
these questions, we looked at a multi-core parallel experiment, and a
peer-to-peer system.

\groupsubsec{5.1 Experiment 1: A Parallel System}

We looked at an existing implementation~\cite{parasols}
of a parallel branch-and-bound
algorithm for the maximum clique problem~\cite{ciaran}: given a graph, a clique
is a subset of pairwise-adjacent vertices, and the maximum clique problem
(which is NP-hard) is to find the largest such subset. On physical hardware, for
any given graph instance, if we run the program multiple times we get very
similar runtimes.  The aim of this experiment is to see whether runtimes are
similarly consistent when run on a VM on a public cloud computing infrastructure: the original
work relies upon performance measurements to explain the behaviour of different
parallelism mechanisms. We did not attempt to reproduce the entire paper;
instead, we devised a new experiment to establish whether meaningful,
consistent parallel performance measurements of the kind required by the
original work could be obtained using the Microsoft Azure cloud~\cite{azure}.

The source code of the implementation was available on GitHub (and was written
by one of the authors). No changes were required, and all of the dependencies
were available pre-packaged on a standard Ubuntu installation.

For our analysis, we selected $10$ different ``medium sized'' problem instances
(i.e., graphs) from the second DIMACS implementation
challenge~\cite{dimacs}, so that our
runtimes would be long enough to be noise-free but short enough to be
repeatable. This is a publicly available dataset, in an easy to parse format,
which is widely used for testing maximum clique algorithms, and there are no
barriers to obtaining or using it.

For each selected instance, we ran our implementation of the algorithm $50$
times on physical hardware (using $4$ cores of a machine with dual Intel Xeon
E5-2640 v2 processors), and then $50$ times on a VM with $4$ cores on Microsoft
Azure. Our measurements include only computation times and ignore the time
taken to read in the problem instances from a file. We compare the coefficient
of variation of the runtimes
on real and virtual hardware for each problem instance.  We present the results
in Table~\ref{run_time}. Each row of the table represents a problem instance.

<<echo=FALSE,results=tex>>=
library(xtable)

data.folder = paste(getwd(),"../Group_2/clique_experiments",sep = "/", collapse = NULL)
data_matrix1 = read.csv(file.path(data.folder,"real.csv"), sep=",", row.names=1)
data_matrix2 = read.csv(file.path(data.folder,"cloud.csv"), sep=",", row.names=1)

data_matrix1<-as.matrix(data_matrix1)
data_matrix2<-as.matrix(data_matrix2)

f <- function(x) c(r_std_dev = sd(x)/mean(x))

result_matrix1 = apply(data_matrix1, 2, f)
result_matrix2 = apply(data_matrix2, 2, f)

run_time <- data.frame(real=result_matrix1,
                                vm=result_matrix2)

xtable(run_time,
       caption = "Coefficient of variation of runtimes for different problem instances, when run on real or virtual hardware. In every case, the value is very low, implies virtualised hardware is as reliable as real hardware.",
       label = "run_time",
       align = "lrr",
       digits = c(0,3,3))
@

In both physical and virtual hardware the coefficient of variation is very small
(at or below $0.01$ in every case). We did not encounter any abnormalities when
running on a VM; this is contrary to the experiences of
Kotthoff~\cite{kotthoff}, who did not always see reliable \emph{sequential}
runtimes on
virtualised hardware. In other words, using virtualised hardware for
reproducible parallelism experiments is not necessarily infeasible.  However,
we were limited to 4 cores, rather than the 64 cores used in the original work.
We are also unsure whether parallel acceleration hardware such as GPUs or the
Intel Xeon Phi could be used in a virtualised environment, and are doubtful
that experiments involving this kind of equipment will be recomputable on later
hardware.

The VM image used for the experiments is available via
VMDepot.\footnote{\url{http://vmdepot.msopentech.com/Vhd/Show?vhdId=44545}}

\groupsubsec{5.2 Experiment 2: A Peer-to-Peer System}
Experiments that are performed across multiple machines are challenging
to reproduce. This is due to the cost of needed resources, and the complexities
involved in configuring the machines, the network, and the relationship between the machines.
Cloud services offer increasingly affordable computation. Such services use
virtualisation to decrease the cost of system (re)configuration.

In order to study the challenges of making distributed experiments reproducible
using cloud services, we deployed a Chord~\cite{chord} Distributed Hash Table
(DHT) of $10$ nodes on $10$ dedicated machines, and tried to reproduce the same
experiment on Microsoft Azure.

Reproducing the same experiment on Microsoft Azure proved to be time consuming 
and we could not reproduce the experiment during the Summer School. We were 
unable to automate a required network-specific port allocation task used in 
this experiment due to lack of time. It highlights the potential complexity of cloud 
computing APIs when being applied in research experiments, as opposed to 
more common deployment scenarios that have more examples and documentation.

It is possible to run experiments across multiple machines reproducible by writing
a vendor-specific script that starts and configures any needed VMs before running
the experiment. This approach raises a number of issues:
\begin{inparaenum}[(i)]
\item it relies on external
services in order to run the experiment,
\item it is time-consuming to produce such an
script, and
\item the script cannot be re-used on other cloud services.
\end{inparaenum}

The use of vendor-agnostic cloud frameworks such as Docker and LibCloud, may 
provide additional flexibility when deploying to different cloud infrastructures. Further 
studies are needed to identify best practices to make distributed experiments reproducible 
using VMs. 
