\section{Case study \#2: recomputing parallel and distributed experiments}

We wish to know whether using a virtualised environment can affect the quality
and reproducibility of parallel and distributed experiments. We looked at a
multi-core parallel experiment, and a peer-to-peer system.

\subsection{Experiment 1: A Parallel System}

We looked at an existing implementation of a parallel algorithm for the maximum
clique problem \cite{ciaran}: given a graph, a clique is a subset of
pairwise-adjacent vertices, and the maximum clique problem (which is NP-hard)
is to find the largest such subset. On real hardware, for any given graph
instance, if we run the program multiple times we get very similar runtimes.
The aim of this experiment is to see whether runtimes are similarly consistent
when run on a virtual machine (VM) on a cloud. For real runtimes (RRT) we used
$4$ cores of a node with $16$ Intel cores (dual Intel Xeon E5-2640 v2 2GHz).
The node has $64$ Gb RAM and has \emph{Scientific Linux $6$} as the operating
system. We have selected $10$ different ``medium sized'' problem instances
(i.e.\ graphs) from the second DIMACS implementation challenge to have a valid
and reasonable comparative analysis. For each instance, we run the algorithm
$50$ times on real harware to collect the RRTs, and then $50$ times on a VM
with $4$ cores on Microsoft Azure \cite{azure} to collect virtual runtimes
(VRTs). We measure runtimes of the algorithm and ignore the input and output
times. We compare the relative standard deviation (standard deviation as a
proportion of the mean, since processing models varies between real and virtual
hardware) of RRTs and VRTs of each problem instance.

We present the result in Table~\ref{run_time}. Each row of the table is a
problem instance. We show the relative standard deviation of RRTs and VRTs,
expressed as a percentage.

<<echo=FALSE,results=tex>>=
library(xtable)

data.folder = paste(getwd(),"../Group_2/clique_experiments",sep = "/", collapse = NULL)
data_matrix1 = read.csv(file.path(data.folder,"real.csv"), sep=",", row.names=1)
data_matrix2 = read.csv(file.path(data.folder,"cloud.csv"), sep=",", row.names=1)

data_matrix1<-as.matrix(data_matrix1)
data_matrix2<-as.matrix(data_matrix2)

f <- function(x) c(r_std_dev = (sd(x)/mean(x))*100)

result_matrix1 = apply(data_matrix1, 2, f)
result_matrix2 = apply(data_matrix2, 2, f)
 
run_time <- data.frame(real=result_matrix1,
                                vm=result_matrix2)

xtable(run_time,
       caption = "Relative Standard Deviations of runtimes (as a percentage)",
       label = "run_time",
       align = "lrr",
       digits = c(0,2,2))
@

In both real and virtual hardware the relative standard deviation is very
small (below $1\%$ in every case). We did not encounter any abnormalities when
running on a virtual machine; this is contrary to the experiences of Kotthoff
\cite{kotthoff}, who did not always see reliable \emph{sequential} runtimes on
virtualised hardware.

The virtual machine image used for the experiments is available via
VMDepot\footnote{\url{http://vmdepot.msopentech.com/Vhd/Show?vhdId=44545}}.

\subsection{Experiment 2: A Peer-to-Peer System}
Experiments that are performed across multiple machines are challenging
to reproduce. This is due to the cost of needed resources, and the complexities
involved in configuring the machines and the relationship between the machines.
Cloud services offer increasingly affordable computation. Such services use
virtualisation to decrease the cost of system reconfiguration.

In order to study the challenges of making distributed experiments reproducible
using cloud services, we deployed a Chord \cite{chord} Distributed Hash Table
(DHT) of $10$ nodes on 10 dedicated machines, and tried to reproduce the same
experiment on Microsoft Azure.

Reproducing the same experiment on Microsoft Azure proved to be time consuming.
Due to the lack of time we were unable to reproduce the experiment. The main reason
is the steep learning curve of using Microsoft Azure's API. The API itself lacks
documentation and is complex in design. Further, since the Azure's API is not as
popular as the rivals, it is harder to find example usage of the API.

It is possible to make experiments across multiple machines reproducible by writing
a vendor-specific script that starts and configures any needed VMs before running
the experiment. This approach raises a number of issues: 1) it relies on external
services in order to run the experiment, 2) it is time-consuming to produce such an
script, and 3) the script cannot be re-used on other cloud services.

Further studies are needed to identify best practices to make distributed experiments
reproducible using virtual machines.

