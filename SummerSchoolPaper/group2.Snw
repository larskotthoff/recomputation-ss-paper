\section{Case study \#2: Recomputing parallel and distributed experiments}

The Recomputation.org project has shown that recomputation via a virtual
machine can been successful when using sequential code on a single
machine.  Our second case study examined 
whether this approach could be extended to cover multi-core
parallel or distributed systems. For parallelism research, the end goal is
typically performance: can using more cores make something run faster, or allow
a larger dataset to be processed in the available time? It is not clear whether
a virtualised environment would affect the quality of the results for these
kinds of experiments. For distributed systems, a single virtual machine would
obviously not suffice, but could multiple virtual machines be used?  To address
these questions, we looked at a multi-core parallel experiment, and a
peer-to-peer system.

\subsection{Experiment 1: A Parallel System}

We looked at an existing implementation \cite{parasols}
of a parallel branch-and-bound
algorithm for the maximum clique problem \cite{ciaran}: given a graph, a clique
is a subset of pairwise-adjacent vertices, and the maximum clique problem
(which is NP-hard) is to find the largest such subset. On real hardware, for
any given graph instance, if we run the program multiple times we get very
similar runtimes.  The aim of this experiment is to see whether runtimes are
similarly consistent when run on a virtual machine on a cloud: the original
work relies upon performance measurements to explain the behaviour of different
parallelism mechanisms. We did not attempt to reproduce the entire paper;
instead, we devised a new experiment to establish whether meaningful,
consistent parallel performance measurements of the kind required by the
original work could be obtained using the Microsoft Azure cloud~\cite{azure}.

The source code of the implementation was available on GitHub (and was written
by one of the authors). No changes were required, and all of the dependencies
were available pre-packaged on a standard Ubuntu installation.

For our analysis, we selected $10$ different ``medium sized'' problem instances
(i.e.\ graphs) from the second DIMACS implementation
challenge,\footnote{\url{http://dimacs.rutgers.edu/Challenges/}} so that our
runtimes would be long enough to be noise-free but short enough to be
repeatable. This is a publicly available dataset, in an easy to parse format,
which is widely used for testing maximum clique algorithms, and there are no
barriers to obtaining or using it.

For each selected instance, we ran our implementation of the algorithm $50$
times on real hardware (using $4$ cores of a machine with dual Intel Xeon
E5-2640 v2 processors), and then $50$ times on a VM with $4$ cores on Microsoft
Azure. Our measurements include only computation times and ignore the time
taken to read in the problem instances from a file. We compare the coefficient
of variation of the runtimes
on real and virtual hardware for each problem instance.  We present the results
in Table~\ref{run_time}. Each row of the table represents a problem instance.

<<echo=FALSE,results=tex>>=
library(xtable)

data.folder = paste(getwd(),"../Group_2/clique_experiments",sep = "/", collapse = NULL)
data_matrix1 = read.csv(file.path(data.folder,"real.csv"), sep=",", row.names=1)
data_matrix2 = read.csv(file.path(data.folder,"cloud.csv"), sep=",", row.names=1)

data_matrix1<-as.matrix(data_matrix1)
data_matrix2<-as.matrix(data_matrix2)

f <- function(x) c(r_std_dev = sd(x)/mean(x))

result_matrix1 = apply(data_matrix1, 2, f)
result_matrix2 = apply(data_matrix2, 2, f)

run_time <- data.frame(real=result_matrix1,
                                vm=result_matrix2)

xtable(run_time,
       caption = "Coefficient of variation of runtimes for different problem instances, when run on real or virtual hardware. In every case, the value is very low, and virtualised hardware is as reliable than real hardware.",
       label = "run_time",
       align = "lrr",
       digits = c(0,3,3))
@

In both real and virtual hardware the coefficient of variation is very small
(at or below $0.01$ in every case). We did not encounter any abnormalities when
running on a virtual machine; this is contrary to the experiences of
Kotthoff~\cite{kotthoff}, who did not always see reliable \emph{sequential}
runtimes on
virtualised hardware. In other words, using virtualised hardware for
reproducible parallelism experiments is not necessarily infeasible.  However,
we were limited to 4 cores, rather than the 64 cores used in the original work.
We are also unsure whether parallel acceleration hardware such as GPUs or the
Intel Xeon Phi could be used in a virtualised environment, and are doubtful
that experiments involving this kind of equipment will be recomputable on later
hardware.

The virtual machine image used for the experiments is available via
VMDepot.\footnote{\url{http://vmdepot.msopentech.com/Vhd/Show?vhdId=44545}}

\subsection{Experiment 2: A Peer-to-Peer System}
Experiments that are performed across multiple machines are challenging
to reproduce. This is due to the cost of needed resources, and the complexities
involved in configuring the machines, the network, and the relationship between the machines.
Cloud services offer increasingly affordable computation. Such services use
virtualisation to decrease the cost of system (re)configuration.

In order to study the challenges of making distributed experiments reproducible
using cloud services, we deployed a Chord~\cite{chord} Distributed Hash Table
(DHT) of $10$ nodes on $10$ dedicated machines, and tried to reproduce the same
experiment on Microsoft Azure.

Reproducing the same experiment on Microsoft Azure proved to be time consuming.
Due to lack of time during the school, we were unable to reproduce the experiment. The main reason
is the steep learning curve of using Microsoft Azure's API. The API itself lacks
documentation and is complex in design. Since Azure is not as
popular as the competitor products, it is harder to find examples of the API
usage.

It is possible to run experiments across multiple machines reproducible by writing
a vendor-specific script that starts and configures any needed VMs before running
the experiment. This approach raises a number of issues:
\begin{inparaenum}[(i)]
\item it relies on external
services in order to run the experiment,
\item it is time-consuming to produce such an
script, and
\item the script cannot be re-used on other cloud services.
\end{inparaenum}

Further studies are needed to identify best practices to make distributed experiments
reproducible using virtual machines.
