\section{Case study \#2: recomputing parallel and distributed experiments}

We wish to know whether using a virtualised environment can affect the quality
and reproducibility of parallel and distributed experiments. We looked at a
multi-core parallel experiment, and a peer-to-peer system.

\subsection{Experiment 1: A Parallel System}

We looked at an existing implementation of a parallel algorithm for the maximum
clique problem \cite{ciaran}: given a graph, a clique is a subset of
pairwise-adjacent vertices, and the maximum clique problem (which is NP-hard)
is to find the largest such subset. On real hardware, for any given graph
instance, if we run the program multiple times we get very similar runtimes.
The aim of this experiment is to see whether runtimes are similarly consistent
when run on a virtual machine on a cloud.

We have selected $10$ different ``medium sized'' problem instances (i.e.\
graphs) from the second DIMACS implementation challenge to have a valid and
reasonable comparative analysis.  For each instance, we run the algorithm $50$
times on real harware (using $4$ cores of a machine with dual Intel Xeon
E5-2640 v2 processors), and then $50$ times on a VM with $4$ cores on Microsoft
Azure \cite{azure}. Our measurements include only computation times, and ignore
the time taken to read in the problem instances from a file. We compare the
relative standard deviation (that is, standard deviation as a proportion of the
mean, since processing models varies between real and virtual hardware) of the
runtimes on real and virtual hardware for each problem instance.  We present
the result in Table~\ref{run_time}: each row of the table is a problem
instance, and we show relative standard deviations as a percentage.

<<echo=FALSE,results=tex>>=
library(xtable)

data.folder = paste(getwd(),"../Group_2/clique_experiments",sep = "/", collapse = NULL)
data_matrix1 = read.csv(file.path(data.folder,"real.csv"), sep=",", row.names=1)
data_matrix2 = read.csv(file.path(data.folder,"cloud.csv"), sep=",", row.names=1)

data_matrix1<-as.matrix(data_matrix1)
data_matrix2<-as.matrix(data_matrix2)

f <- function(x) c(r_std_dev = (sd(x)/mean(x))*100)

result_matrix1 = apply(data_matrix1, 2, f)
result_matrix2 = apply(data_matrix2, 2, f)

run_time <- data.frame(real=result_matrix1,
                                vm=result_matrix2)

xtable(run_time,
       caption = "Relative standard deviations of runtimes (as a percentage) for different problem instances, when run on real or virtual hardware.",
       label = "run_time",
       align = "lrr",
       digits = c(0,2,2))
@

In both real and virtual hardware the relative standard deviation is very small
(below $1\%$ in every case). We did not encounter any abnormalities when
running on a virtual machine; this is contrary to the experiences of Kotthoff
\cite{kotthoff}, who did not always see reliable \emph{sequential} runtimes on
virtualised hardware. In other words, using virtualised hardware for
reproducible parallelism experiments is not necessarily infeasible.

The virtual machine image used for the experiments is available via
VMDepot\footnote{\url{http://vmdepot.msopentech.com/Vhd/Show?vhdId=44545}}.

\subsection{Experiment 2: A Peer-to-Peer System}
Experiments that are performed across multiple machines are challenging
to reproduce. This is due to the cost of needed resources, and the complexities
involved in configuring the machines and the relationship between the machines.
Cloud services offer increasingly affordable computation. Such services use
virtualisation to decrease the cost of system reconfiguration.

In order to study the challenges of making distributed experiments reproducible
using cloud services, we deployed a Chord \cite{chord} Distributed Hash Table
(DHT) of $10$ nodes on $10$ dedicated machines, and tried to reproduce the same
experiment on Microsoft Azure.

Reproducing the same experiment on Microsoft Azure proved to be time consuming.
Due to the lack of time we were unable to reproduce the experiment. The main reason
is the steep learning curve of using Microsoft Azure's API. The API itself lacks
documentation and is complex in design. Since the Azure's API is not as
popular as the rivals, it is harder to find example usage of the API.

It is possible to make experiments across multiple machines reproducible by writing
a vendor-specific script that starts and configures any needed VMs before running
the experiment. This approach raises a number of issues: 1) it relies on external
services in order to run the experiment, 2) it is time-consuming to produce such an
script, and 3) the script cannot be re-used on other cloud services.

Further studies are needed to identify best practices to make distributed experiments
reproducible using virtual machines.
