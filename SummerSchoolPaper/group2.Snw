\section{Case study \#2: recomputing parallel and distributed experiments}

We wish to know whether using a virtualised environment can affect the quality
and reproducibility of parallel and distributed experiments. We looked at a
multi-core parallel experiment, and a peer-to-peer system.

\subsection{Experiment 1: A Parallel System}

We looked at an existing implementation of a parallel algorithm for the maximum
clique problem \cite{ciaran}: given a graph, a clique is a subset of
pairwise-adjacent vertices, and the maximum clique problem (which is NP-hard)
is to find the largest such subset. On real hardware, for any given graph
instance, if we run the program multiple times we get very similar runtimes.
The aim of this experiment is to see whether runtimes are similarly consistent
when run on a virtual machine (VM) on a cloud. For real runtimes (RRT) we used
$4$ cores of a node with $16$ Intel cores (dual Intel Xeon E5-2640 v2 2GHz).
The node has $64$ Gb RAM and has \emph{Scientific Linux $6$} as the operating
system. We have selected $10$ different ``medium sized'' problem instances
(i.e.\ graphs) from the second DIMACS implementation challenge to have a valid
and reasonable comparative analysis. For each instance, we run the algorithm
$50$ times on real harware to collect the RRTs, and then $50$ times on a VM
with $4$ cores on Microsoft Azure \cite{azure} to collect virtual runtimes
(VRTs). We measure runtimes of the algorithm and ignore the input and output
times. We compare the relative standard deviation (standard deviation as a
proportion of the mean, since processing models varies between real and virtual
hardware) of RRTs and VRTs of each problem instance.

We present the result in Table~\ref{run_time}. Each row of the table is a
problem instance. We show the relative standard deviation of RRTs and VRTs,
expressed as a percentage.

<<echo=FALSE,results=tex>>=
library(xtable)

data.folder = paste(getwd(),"../Group_2/clique_experiments",sep = "/", collapse = NULL)
data_matrix1 = read.csv(file.path(data.folder,"real.csv"), sep=",", row.names=1)
data_matrix2 = read.csv(file.path(data.folder,"cloud.csv"), sep=",", row.names=1)

data_matrix1<-as.matrix(data_matrix1)
data_matrix2<-as.matrix(data_matrix2)

f <- function(x) c(r_std_dev = (sd(x)/mean(x))*100)

result_matrix1 = apply(data_matrix1, 2, f)
result_matrix2 = apply(data_matrix2, 2, f)
 
run_time <- data.frame(real=result_matrix1,
                                vm=result_matrix2)

xtable(run_time,
       caption = "Relative Standard Deviations of runtimes (as a percentage)",
       label = "run_time",
       align = "lrr",
       digits = c(0,2,2))
@

In both real and virtual hardwares the relative standard deviation is very
small (below $1\%$ in every case). We did not encounter any abnormalities when
running on a virtual machine; this is contrary to the experiences of Kotthoff
\cite{kotthoff}, who did not always see reliable \emph{sequential} runtimes on
virtualised hardware.

The virtual machine image used for the experiments is available via
VMDepot\footnote{\url{http://vmdepot.msopentech.com/Vhd/Show?vhdId=44545}}.

\subsection{Experiment 2: A Peer-to-Peer System}

We have used Chord (\cite{chord1}, \cite{chord2}) as the Peer-to-Peer (P2P)
system to run our experiments. To satisfy the pupose of this report, we run
each experiment once on real hardware and on the Virtual Machines (VMs) on
Microsoft Azure \cite{azure}. We have created a network of 12 nodes, each
running on a seperate machine, which is of $4$ cores ($4$*Intel Xeon 2GHz) with
$8$ GBs RAM and Linux $2.6$ as the operating system. To have a valid comparison
we have created $12$ VMs on Microsoft Azure, each VM runs on single core. 

A \emph{Workload} model is a temporal sequence of lookups with exponentially
distributed intervals. The studied workload models are: i) light (mean interval
of 10 seconds), and ii) heavy (mean inverval of $1$ second). A \emph{Churn}
model is a temporal sequence of node arrivals and departures from the network.
The session lengths and down times of nodes are exponentially distributed. The
studied churn models are: i) none (nodes stay alive throughout the experiment),
ii) light (mean session length and downtime is $10$ minutes) and iii) heavy
(mean session length and downtime is $1$ minute). We run each experiment for
$1$ hour and repeat each experiment $5$ times. 

