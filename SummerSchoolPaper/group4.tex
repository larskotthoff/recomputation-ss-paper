\section{Case study \#4: Recomputability of HCI Studies}
\label{s:group4}

Research in computer science often involves humans, especially if the subject of study is how people and machines interact with each other (e.g., in HCI). This kind of research focuses on phenomena that involve human agents and are therefore not addressable strictly through computation. However, there are also strictly computational issues that are critical for the replicability of HCI research. In this case study we looked at current practice in HCI research through an example published experiment, and aimed at identifying the road blocks and difficulties that present themselves when reproducing statistical analysis of data obtained from human behavior.  

\subsection{Background}
The field of Human-computer Interaction heavily relies on the execution and analysis of empirical studies that involve humans. These results are used to, for example, build new interaction techniques and devices\cite{}, and propose theories of human behavior with computers \cite{}. The reliability of the analysis and conclusions of these studies have been recently questioned. Problems identified include the fact that barely any results are replicated \cite{hornbaek:replications}, that the research is often difficult or impossible to replicate \cite{wilson1}, and that replicated results are difficult to publish and share to the larger community \cite{}. The HCI community is currently trying to address some of these problems through new venues for publication (e.g., ReplyCHI~\cite{replichiVenue}), the creation of new tools~\cite{mackay}, and efforts to change the research culture and incentives (ACM CHI, arguably the most important conference in the field, introduced in 2013 a replication award or distinction for papers that address replicability).

Although the problem of replicability of experiments with humans is difficult and will likely require significant efforts from the community, the recomputability of these results and the associated statistical analysis has received relatively little attention, even though it is probably one of the most significant sources of inaccuracy and incorrect data in the field. Recomputability in HCI experiments refers mostly to the ability of others (not authors) to replicate the statistical analysis and statistical conclusions of a paper utilizing the same recorded data from the experiment. 

Quantitative experiments in HCI analyze data that is obtained from humans to draw conclusions that are relevant for the understanding or development of interfaces. Although the experiments are necessarily affected by the inherent variability introduced by humans, the analyses should not. Ideally, every researcher in the area, and more specifically, every reviewer of a paper containing statistical analysis of quantitative human data should be able to reproduce the analysis. The ability of reviewers to determine if a statistical analysis and interpretation of the data in a paper are correct is currently limited to checking that the reported degrees of freedom in an ANOVA (or a similar inferential statistic procedure) are consistent with the design of the experiment, and that the intermediate statistic figures (e.g., F values, DOF, p-values) are consistent with the statistic analysis. This is obviously not sufficient to detect even relatively simple errors during analysis that could mean the difference between radically opposite interpretations of the data. Examples that have been encountered by some of the authors of this paper include: reading statistics and degrees of freedom from an incorrect column in the software, reading statistics and degrees of freedom from an incorrect table, and performing within-subject analysis on between-subjects data. Some of these errors are virtually impossible to detect if the only provided information are the statistical figures typically found in papers. The problem is further magnified if the analysis is not standard. For example, if a new computational measure is created from the data, it might be impossible to reproduce without having the exact code, and if the analysis applies a machine learning approach there might be large numbers of parameters to adjust and many differently implemented variants of the same analysis (different analysis frameworks might have implementations of the same analysis that might lead to different results).

In order to prevent those errors and the significant loss of credibility of the data that they cause, authors should enable the recomputation of statistical and machine learning analysis on the data of any experiment, to the extent allowed by other ethics and privacy issues (See section XXXX). This requires that: a) authors make the data available, b) authors provide suitable meta-data that describes the semantics and structure of the data, c) authors provide instruction on how to reproduce the analysis. Sharing the data and the procedures of the analysis has advantages that go beyond the pure verifiability of the correctness of the result: the data can also be reanalysed (individually or in combination with other sources) to discover new insights, the analysis can serve as educational material for students in the area, and scientific fraud becomes, at least in theory, much harder to perpetrate. 

In this spirit of openness and scientific integrity, one of the authors (M.A.N.) has been striving to provide the data and the analyses for his own empirical research in HCI. Specifically, a recent research project on the memorability of gestures \cite{Nacenta:memorability} was developed from scratch as a pilot experience that would enable anyone to reproduce the analysis. For this purpose, the data and the basic analysis scripts necessary to perform the inferential statistics contained in the paper were prepared and included as an attachment to the original paper, which is currently accessible through the institutional research repository at the University of St Andrews~\cite{Nacenta:memorability_data}. This data and the required auxiliary files took approximately 6 hours to compile and prepare by the main author (excluding the time spent compiling and designing the statistical analyses). If this paper is representative of other work in the area, this amount of effort on the side of the authors does seem reasonable in exchange for the expected quality improvements for the field that recomputability could deliver. However, we have little knowledge about the challenges and difficulties encountered by the replicators (rather than the authors) in order to verify and check that the analysis is correct.

For this purpose, and in the context of the summer school that this article reflects on, we decided to set up an experiment in which the participants of the summer school (and authors of this article) with the exception of the author of the data, would try to replicate the results of the paper. The main objective of this research is to learn about the challenges and difficulties of a simple recomputation exercise of standard statistical analysis, to provide real examples of experience in recomputation of analysis in HCI, and to enable improvement of the provided data in the future.

\subsection{Experience Report: Recomputing a Memorability Experiment}
The authors of this article (hence the \emph{reanalysts}), with the exception of
M.A.N. divided themselves into four teams (4, 3, 4, and 5 people per team), each of which would try to reproduce the same selection of results of the gesture memorability study reported in reference \cite{Nacenta:memorability}. The target results for reanalysis were the averages and ANOVA analyses of the first paragraph of the /emph{Results} section of \emph{Experiment 3}. This paragraph contained three types of analysis: simple calculations of averages (recall rates), omnibus parametric ANOVA analyses, and pairwise post-hoc parametric t-tests. Approximately half of the reanalysts had a good understanding of HCI or had performed research in the HCI field. To provide sufficient background, the author of the reanalyzed paper gave a 20 minute presentation on the content of the paper, aimed at a moderately knowledgeable audience. Reanalysts were allowed to ask any number of questions at the end.

The reanalysts received also a physical and a digital version of the original paper and a URL from where to download the data (as distributed originally to the public in \cite{Nacenta:memorability_data}). The data is provided in a comma separated file (with column heading names in the first row). The data package also includes IBM SPSS Syntax files (SPSS's scripting language), and a README.txt file that contains descriptions of the different files, including explanations of the meanings of columns. SPSS syntax files were provided because it was the platform in which the analyses were performed, and it is commonly used as statistical software for the analysis of experiments in the HCI and Psychology communities. 

Teams were given approximately 90 minutes to replicate the results contained in the paragraph of the paper indicated above. Of the four groups, two opted to try to replicate the results by using SPSS (installed in the machines available to the reanalysts), one opted to replicate the results using R, and one opted to replicate the results using R while simultaneously recording the recomputation in a VM. The leader of the session provided help to the SPSS groups strictly on issues related to the SPSS interface. Each group was asked to assign one person to take notes on a paper notepad of the development of the session (specifically, steps taken, difficulties found, misunderstandings, and breakthroughs).

\subsection{Results}
All the reanalysts spent the allocated time (approximately 90 minutes) working on the recomputation of the results while recording on their notepads the actions and obstacles encountered. After the session was over, the reanalysts shared in public their results, conclusions and main obstacles for the benefit of the rest of the groups. The notepads were later analyzed by M.A.N. by identifying problems, creating a physical affinity diagram of problems \cite{GUIBook}, and identifying the most relevant groups of related problems. The two following subsections report the degree of success achieved in the recomputation and the main categories of challenges and obstacles found.

\subsubsection{Measures of Success}
Groups 2, 3 and 4 were able to achieve some kind of verification of data present in the paper in the time allocated.

\emph{Group 1 (SPSS)} were able to open the data, read the README file, and run the script that loads and performs the analysis. However, they were not able to find the appropriate correspondence between results on the paper and the output of SPSS. 
\emph{Group 2 (SPSS)} were able to open the data, read the README file, load the data with SPSS independently of the SPSS script, verify the integrity and structure of the data, run the scripts, find one of the ANOVA analyses in the output, and verify its correctness.
\emph{Group 3 (R)} were able to find and open the data, read the data with R, and run some basic descriptive statistics (averages).
\emph{Group 4 (R + VM)} were able to create a VM to store the analysis of the data, unpack the data, read the README file, failed at converting the provided SPSS Syntax scripts into R scripts, but were finally able to reproduce some basic descriptive statistics (averages).

\subsubsection{Identified Problems and Challenges}
This section summarizes the four main groups of problems and challenges identified, namely: tool problems (SPSS), cross-tool problems, data and script problems, and lacks of knowledge.

\emph{Tool problems.} Reanalysts found it difficult to load data in SPSS, to run scripts, and and found the syntax scripts themselves non human-readable. The SPSS model of running scripts and presenting the results in very long report in a separate window/file was also found confusing. The SPSS Syntax Scripting facility is also difficult to get to work, and can be misleading (the system is not designed with the main goal of running full scripts), even for previous users of the tool. Additionally, the scripts cannot use relative file references, which forces the reanalysts to change the script itself instead of just running it (the folder structure of the reanalysis machine is not necessarily the same as the original machine where the data was first analysed). 

\emph{Cross-tool problems.} Reanalysts were unsure of whether the difference in versions from the software used for the original analysis (SPSS 19), and the tool available for reanalysis (SPSS 21) would cause problems. One group that felt comfortable with R but wanted to take advantage of the provided SPSS Syntax scripts tried to convert one to the other using an existing free tool XXXX\cite{Ian's Group Should find reference for this tool}XXXX, however, the tool was found to be inadequate for this purpose; conversion from one language in one tool to another is a very complex problem, not likely to be solved soon. Additionally, the necessary installation of packages, dependencies, and the VM caused significant overhead.

\emph{Data and Script problems.} The data and scripts provided were also not ideal, and generated a number of problems and difficulties. Reanalysts detected inconsistencies in the naming of conditions and columns between the data and the paper, which are due to the authors of the original paper renaming conditions and columns to make the paper more readable. Some groups also tried to identify data based on the SPSS-generated graphics, but these do not correspond to the graphics used in the final version of the paper (SPSS graphics are not of the quality and format required in most scientific publications, and therefore had to be redone). This caused confusion to three groups. Finally, the analyses provided in the SPSS Syntax are exhaustive, containing much more information than the paper. This caused confusion in reanalysts, who had serious difficulties relating the output generated by the scripts with the data reported in the paper. This was sometimes made worse by the fact that the other was different in both systems.

\emph{Required Knowledge Breadth.} All groups highlighted the depth and breadth of knowledge required to achieve recomputation of data. At the lowest levels of abstraction, reanalysts had to be knowledgeable in SPSS operation. Knowledge on the use of data formats is also a requirement. Those groups that used R for analysis did not only have to show a significant mastery of R, but also of the relationship between R and SPSS Syntax and, more importantly, of the specific statistical procedures and how they are performed in both platforms. Finally, the reanalysts had to achieve a grounded understanding of the experimental design and purpose of the experiment, something that requires detailed and thoughtful study.

\subsection{Discussion and Recommendations}
Although the main focus to achieve replicability in HCI has focused on the replication of empipirical data collection, there is still much to do (and much benefit to get) from improving the recomputability of the analyses of the data gathered. In this section we discuss the main issues and lessons learnt from our experience, as well as limitations from our methods and a set of recommendations on how to improve the impact and feasibility of recomputation in HCI.

\subsubsection{Reasonable Success}

\subsubsection{Tools are Key (and not ready)}

\subsubsection{A Culture Change}

SPSS might be an adequate tool for performing statistical analysis; it is successfully used by many in HCI and many other areas. However, the reanalysts had many problems with the tool, some related to the general usability of the tool, some related to the implicit design assumption in SPSS that the data is collected, analized and interpreted by the same person.

Do not underestimate the effort of collecting and preparing the analysis for other's consumption. Specifically, the analysis should probably be re-written, adjusting for new names and variables contained in the paper.

- Reasonable performance for a 90 minute. Feasible, makes the effort worth it.
- Not perfect - tools not ideal for this task
- A good recomputation set is like a program: needs to have comments and an easily traceable structure.
- Additionaly, problematic to 

- Problems with the amounts of the analysis
- Problems with the design of the software (SPSS)

\subsubsection{Limitations}

- Limitations in this experience

- Layers of knowledge needed pretty broad
- Access to software problematic (SPSS)
- Specific numbers difficult to find
- Consistency between 


\subsubsection{Recommendations}

