\section{Case study \#4: Recomputability of HCI Studies}
\label{s:group4}

Research in computer science often involves humans, especially if the subject of study is how people and machines interact with each other (e.g., in HCI). This kind of research focuses on phenomena that involve human agents and are therefore not addressable strictly through computation. However, there are also strictly computational issues that are critical for the replicability of HCI research. In this case study we looked at current practice in HCI research through an example published experiment, and aimed at identifying the road blocks and difficulties that present themselves when reproducing statistical analysis of data obtained from human behavior.  

\subsection{Background}
The field of Human-computer Interaction heavily relies on the execution and analysis of empirical studies that involve humans. These results are used to, for example, build new interaction techniques and devices\cite{}, and propose theories of human behavior with computers \cite{}. The reliability of the analysis and conclusions of these studies have been recently questioned. Problems identified include the fact that barely any results are replicated \cite{hornbaek}, that the research is often difficult or impossible to replicate \cite{wilson1}, and that replicated results are difficult to publish and share to the larger community \cite{}. The HCI community is currently trying to address some of these problems through new venues for publication (e.g., ReplyCHI~\cite{replichiVenue}), the creation of new tools~\cite{mackay}, and efforts to change the research culture and incentives (ACM CHI, arguably the most important conference in the field, introduced in 2013 a replication award or distinction for papers that address replicability).

Although the problem of replicability of experiments with humans is difficult and will likely require significant efforts from the community, the recomputability of these results and the associated statistical analysis has received relatively little attention, even though it is probably one of the most significant sources of inaccuracy and incorrect data in the field. Recomputability in HCI experiments refers mostly to the ability of others (not authors) to replicate the statistical analysis and statistical conclusions of a paper utilizing the same recorded data from the experiment. 

Quantitative experiments in HCI analyze data that is obtained from humans to draw conclusions that are relevant for the understanding or development of interfaces. Although the experiments are necessarily affected by the inherent variability introduced by humans, the analyses should not. Ideally, every researcher in the area, and more specifically, every reviewer of a paper containing statistical analysis of quantitative human data should be able to reproduce the analysis. The ability of reviewers to determine if a statistical analysis and interpretation of the data in a paper are correct is currently limited to checking that the reported degrees of freedom in an ANOVA (or a similar inferential statistic procedure) are consistent with the design of the experiment, and that the intermediate statistic figures (e.g., F values, DOF, p-values) are consistent with the statistic analysis. This is obviously not sufficient to detect even relatively simple errors during analysis that could mean the difference between radically opposite interpretations of the data. Examples that have been encountered by some of the authors of this paper include: reading statistics and degrees of freedom from an incorrect column in the software, reading statistics and degrees of freedom from an incorrect table, and performing within-subject analysis on between-subjects data. Some of these errors are virtually impossible to detect if the only provided information are the statistical figures typically found in papers. The problem is further magnified if the analysis is not standard. For example, if a new computational measure is created from the data, it might be impossible to reproduce without having the exact code, and if the analysis applies a machine learning approach there might be large numbers of parameters to adjust and many differently implemented variants of the same analysis (different analysis frameworks might have implementations of the same analysis that might lead to different results).

In order to prevent those errors and the significant loss of credibility of the data that they cause, authors should enable the recomputation of statistical and machine learning analysis on the data of any experiment, to the extent allowed by other ethics and privacy issues (See section XXXX). This requires that: a) authors make the data available, b) authors provide suitable meta-data that describes the semantics and structure of the data, c) authors provide instruction on how to reproduce the analysis. Sharing the data and the procedures of the analysis has advantages that go beyond the pure verifiability of the correctness of the result: the data can also be reanalysed (individually or in combination with other sources) to discover new insights, the analysis can serve as educational material for students in the area, and scientific fraud becomes, at least in theory, much harder to perpetrate. 

In this spirit of openness and scientific integrity, one of the authors (M.A.N.) has been striving to provide the data and the analyses for his own empirical research in HCI. Specifically, a recent research project on the memorability of gestures \cite{Nacenta:memorability} was developed from scratch as a pilot experience that would enable anyone to reproduce the analysis. For this purpose, the data and the basic analysis scripts necessary to perform the inferential statistics contained in the paper were prepared and included as an attachment to the original paper, which is currently accessible through the institutional research repository at the University of St Andrews~\cite{Nacenta:memorability_data}. This data and the required auxiliary files took approximately 6 hours to compile and prepare by the main author (excluding the time spent compiling and designing the statistical analyses). If this paper is representative of other work in the area, this amount of effort on the side of the authors does seem reasonable in exchange for the expected quality improvements for the field that recomputability could deliver. However, we have little knowledge about the challenges and difficulties encountered by the replicators (rather than the authors) in order to verify and check that the analysis is correct.

For this purpose, and in the context of the summer school that this article reflects on, we decided to set up an experiment in which the participants of the summer school (and authors of this article) with the exception of the author of the data, would try to replicate the results of the paper. The main objective of this research is to learn about the challenges and difficulties of a simple recomputation exercise of standard statistical analysis, to provide real examples of experience in recomputation of analysis in HCI, and to enable improvement of the provided data in the future.

\subsection{Experience Report: Recomputing a Memorability Experiment}
The authors of this article (hence the \emph{reanalysts}), with the exception of
M.A.N. divided themselves into four groups (4, 4, 5, and 3 people per group), each of which would try to reproduce the same selection of results of the gesture memorability study reported in reference \cite{Nacenta:memorability}. The target results for reanalysis were the averages and ANOVA analyses of the first paragraph of the <emph{Results} section of \emph{Experiment 3}. Approximately half of the reanalysts had a good understanding of HCI or had performed research in the HCI field. To provide sufficient background, the author of the reanalyzed paper gave a 20 minute presentation on the content of the paper, aimed at a moderately knowledgeable audience. Reanalysts were allowed to ask any number of questions at the end.

The reanalysts received also a physical and a digital version of the original paper and a URL from where to download the data (as distributed originally to the public in \cite{Nacenta:memorability_data}). The data is provided in a comma separated file (with column heading names in the first row). The data package also includes IBM SPSS Syntax files (SPSS's scripting language), and a README.txt file that contains descriptions of the different files, including explanations of the meanings of columns. SPSS syntax files were provided because it was the platform in which the analyses were performed, and it is commonly used as statistical software for the analysis of experiments in the HCI and Psychology communities. 

Groups were given approximately 90 minutes to replicate the results contained in the paragraph of the paper indicated above. Of the four groups, two opted to try to replicate the results by using SPSS (installed in the machines available to the reanalysts), one opted to replicate the results using R, and one opted to replicate the results using R while simultaneously recording the recomputation in a virtual machine. The leader of the session provided help to the SPSS groups strictly on issues related to the SPSS interface. Each group was asked to assign one person to take notes on a paper notepad of the development of the session (specifically, steps taken, difficulties found, misunderstandings, and breakthroughs).

\subsection{Results}

\subsubsection{Effectiveness}
\emph{Group 1 (SPSS)} achieved 
\emph{Group 2 (SPSS)} achieved
\emph{Group 3 (SPSS)} achieved
\emph{Group 4 (SPSS)} achieved

\subsubsection{Difficulties and Challenges}


- Layers of knowledge needed pretty broad
- Access to software problematic (SPSS)
- Specific numbers difficult to find
- Consistency between 

\subsection{Discussion}

\subsubsection{Challenges}

\subsubsection{Recommendations}





\begin{figure}
\begin{minipage}{\linewidth}
\includegraphics{group1-001}
\caption{\label{p:ethicsFreq}Histogram showing frequency distribution of fields in ethics applications}
\end{minipage}
\end{figure}

Figure~\ref{p:ethicsFreq} shows the distribution of attributes 
