\section{Case study \#4: Recomputability of HCI Studies}
\label{s:group4}

Research in computer science often involves humans, especially if the subject of study is how people and machines interact with each other (e.g., in HCI). This kind of research focuses on phenomena that involve human agents and are therefore not addressable strictly through computation. However, there are also strictly computational issues that are critical for the replicability of HCI research. In this case study we looked at current practice in HCI research through an example published experiment, and aimed at identifying the road blocks and difficulties that present themselves when reproducing statistical analysis of data obtained from human behavior.  

\subsection{Background}
The field of HCI heavily relies on the execution and analysis of empirical studies that involve humans. These results are used to, for example, build new interaction techniques and devices~\cite{Nacenta:2008}, and propose new models of human behavior with computers~\cite{Shoemaker:2012}. The reliability of the analysis and conclusions of these studies have been recently questioned. Problems identified include the fact that barely any results are replicated~\cite{hornbaek:replications}, that the research is often difficult or impossible to replicate~\cite{wilson:2011}, and that replicated results are difficult to publish and share to the larger community~\cite{wilson:2012}. The HCI community is currently trying to address some of these problems through new venues for publication (e.g., RepliCHI~\cite{wilson:2013,wilson:2014}), the creation of new tools~\cite{Mackay:2007}, and efforts to change the research culture and incentives (ACM CHI, arguably the most important conference in the field, introduced in 2013 a replication award or distinction for papers that address replicability).

Although the problem of replicability of experiments with humans is difficult and will likely require significant efforts from the community, the recomputability of these results and the associated statistical analysis has received relatively little attention, even though it is probably one of the most significant sources of inaccuracy and incorrect data in the field. Recomputability in HCI experiments refers mostly to the ability of others (not authors) to replicate the statistical analysis and statistical conclusions of a paper utilizing the same recorded data from the experiment. 

Quantitative experiments in HCI analyse data that is obtained from humans to draw conclusions that are relevant for the understanding or development of interfaces. Although the experiments are necessarily affected by the inherent variability introduced by humans, the analyses should not. Ideally, every researcher in the area, and more specifically, every reviewer of a paper containing statistical analysis of quantitative human data should be able to reproduce the analysis. The ability of reviewers to determine if a statistical analysis and interpretation of the data in a paper are correct is currently limited to checking that the reported degrees of freedom in an ANOVA (or a similar inferential statistic procedure) are consistent with the design of the experiment, and that the intermediate statistic figures (e.g., F values, DOF, p-values) are consistent with the statistic analysis. This is obviously not sufficient to detect even relatively simple errors during analysis that could mean the difference between radically opposite interpretations of the data. Examples that have been encountered by some of the authors of this paper include: reading statistics and degrees of freedom from an incorrect column in the software, reading statistics and degrees of freedom from an incorrect table, and performing within-subject analysis on between-subjects data. Some of these errors are virtually impossible to detect if the only provided information are the statistical figures typically found in papers. The problem is further magnified if the analysis is not standard. For example, if a new computational measure is created from the data, it might be impossible to reproduce without having the exact code, and if the analysis applies a machine learning approach there might be large numbers of parameters to adjust and many differently implemented variants of the same analysis (different analysis frameworks might have implementations of the same analysis that might lead to different results).

In order to prevent those errors and the significant loss of credibility of the data that they cause, authors should enable the recomputation of statistical and machine learning analysis on the data of any experiment, to the extent allowed by other ethics and privacy issues (Section~\ref{s:group1}). This requires that: a) authors make the data available, b) authors provide suitable meta-data that describes the semantics and structure of the data, c) authors provide instruction on how to reproduce the analysis. Sharing the data and the procedures of the analysis has advantages that go beyond the pure verifiability of the correctness of the result: the data can also be reanalysed (individually or in combination with other sources) to discover new insights, the analysis can serve as educational material for students in the area, and scientific fraud becomes, at least in theory, much harder to perpetrate. 

In this spirit of openness and scientific integrity, one of the authors (M.A.N.) has been striving to provide the data and the analyses for his own empirical research in HCI. Specifically, a recent project on the memorability of gestures~\cite{Nacenta:memorability} was developed from scratch as a pilot experience that would enable anyone to reproduce the analysis. For this purpose, the data and the basic analysis scripts necessary to perform the inferential statistics contained in the paper were prepared and included as an attachment to the original paper, which is currently accessible through the institutional research repository at the University of St Andrews~\cite{Nacenta:memorability_data}. This data and the required auxiliary files took approximately 6 hours to compile and prepare by the main author (excluding the time spent compiling and designing the statistical analyses). If this paper is representative of other work in the area, this amount of effort on the side of the authors does seem reasonable in exchange for the expected quality improvements for the field that recomputability could deliver. However, we have little knowledge about the challenges and difficulties encountered by the replicators (rather than the authors) in order to verify and check that the analysis is correct.

For this purpose, and in the context of the summer school that this article reflects on, we decided to set up an experiment in which the participants of the summer school (and authors of this article) with the exception of the author of the data, would try to replicate the results of the paper. The main objective of this research is to learn about the challenges and difficulties of a simple recomputation exercise of standard statistical analysis, to provide real examples of experience in recomputation of analysis in HCI, and to enable improvement of the provided data in the future.

\subsection{Experience Report: Recomputing a Memorability Experiment}
The authors of this article (henceforth the \emph{reanalysts}), with the exception of
M.A.N. divided themselves into four teams (4, 3, 4, and 5 people per team), each of which would try to reproduce the same selection of results of the gesture memorability study reported in reference~\cite{Nacenta:memorability}. The target results for reanalysis were the averages and ANOVA analyses of the first paragraph of the /emph{Results} section of \emph{Experiment 3}. This paragraph contained three types of analysis: simple calculations of averages (recall rates), omnibus parametric ANOVA analyses, and pairwise post-hoc parametric t-tests. Approximately half of the reanalysts had a good understanding of HCI or had performed research in the HCI field. To provide sufficient background, the author of the reanalyzed paper gave a 20-minute presentation on the content of the paper, aimed at a moderately knowledgeable audience. Reanalysts were allowed to ask any number of questions at the end.

The reanalysts received also a physical and a digital version of the original paper and a URL from where to download the data (as distributed originally to the public in~\cite{Nacenta:memorability_data}). The data is provided in a comma separated file (with column heading names in the first row). The data package also includes IBM SPSS Syntax files (SPSS's scripting language), and a README.txt file containing descriptions of the different files, including explanations of the columns. SPSS Syntax files were provided because it was the platform in which the analyses were performed, and it is commonly used as statistical software for the analysis of experiments in the HCI and Psychology communities. 

Teams were given approximately 90 minutes to replicate the results contained in the paragraph of the paper indicated above. Two groups opted to try to replicate the results by using SPSS (installed in the machines available to the reanalysts), one opted to replicate the results using R, and one opted to replicate the results using R while simultaneously recording the recomputation in a VM. The leader of the session provided help to the SPSS groups strictly on issues related to the SPSS interface. Each group was asked to assign one person to take notes on a paper notepad of the development of the session (specifically, steps taken, difficulties found, misunderstandings, and breakthroughs).

\subsection{Results}
All the reanalysts spent the allocated time working on the recomputation of the results while recording on their notepads the actions and obstacles encountered. After the session was over, the reanalysts shared in public their results, conclusions and main obstacles for the benefit of the rest of the groups. The notepads were later analyzed by M.A.N. by identifying problems, creating a physical affinity diagram of problems~\cite{hartson:2012}, and identifying the most relevant groups of related problems. The two following subsections report the degree of success achieved in the recomputation and the main categories of challenges and obstacles found.

\subsubsection{Measures of Success}
Groups 2, 3 and 4 were able to achieve some verification of data present in the paper in the allotted time.

\emph{Group 1 (SPSS)} were able to open the data, read the README file, and run the script that loads and performs the analysis. They were not, however, able to find the appropriate correspondence between results on the paper and the output of SPSS. 
\emph{Group 2 (SPSS)} were able to open the data, read the README file, load the data with SPSS independently of the SPSS script, verify the integrity and structure of the data, run the scripts, find one of the ANOVA analyses in the output, and verify its correctness.
\emph{Group 3 (R)} were able to find and open the data, read the data with R, and run some basic descriptive statistics (averages).
\emph{Group 4 (R + VM)} were able to create a VM to store the analysis of the data, unpack the data, read the README file, failed at converting the provided SPSS Syntax scripts into R scripts, but were finally able to reproduce some basic descriptive statistics (averages).

\subsubsection{Identified Problems and Challenges}
We identified four main groups of problems and challenges: tool problems, cross-tool problems, data and script problems, and lacks of knowledge.

\emph{Tool problems.} Reanalysts found it difficult to load data in SPSS, to run scripts, and and found the syntax scripts themselves non human-readable. The SPSS model of running scripts and presenting the results in very long report in a separate window/file was also found confusing. The SPSS Syntax Scripting facility is also difficult to get to work, and can be misleading (the system is not designed with the main goal of running full scripts), even for previous users of the tool. Additionally, the scripts cannot use relative file references, which forces the reanalysts to change the script itself instead of just running it (the folder structure of the reanalysis machine is not necessarily the same as the original machine where the data was first analysed). 

\emph{Cross-tool problems.} Reanalysts were unsure of whether the difference in
versions from the software used for the original analysis (SPSS 19), and the
tool available for reanalysis (SPSS 21) would cause problems. One group that
felt comfortable with R but wanted to take advantage of the provided SPSS Syntax
scripts tried to convert one to the other using an existing free
R package~\cite{spsstor}, however, the tool was found to be inadequate for this purpose; conversion from one language in one tool to another is a very complex problem, not likely to be solved soon. Additionally, the necessary installation of packages, dependencies, and the VM caused significant overhead.

\emph{Data and Script problems.} The data and scripts provided were also not ideal, and generated a number of problems and difficulties. Reanalysts detected inconsistencies in the naming of conditions and columns between the data and the paper, which are due to the authors of the original paper renaming conditions and columns to make the paper more readable. Some groups also tried to identify data based on the SPSS-generated graphics, but these do not correspond to the graphics used in the final version of the paper (SPSS graphics are not of the quality and format required in most scientific publications, and therefore had to be redone). This caused confusion to three groups. Finally, the analyses provided in the SPSS Syntax are exhaustive, containing much more information than the paper. This caused confusion in reanalysts, who had serious difficulties relating the output generated by the scripts with the data reported in the paper. This was sometimes made worse by the fact that the other was different in both systems.

\emph{Required Knowledge Breadth.} All groups highlighted the depth and breadth of knowledge required to achieve recomputation of data. At the lowest levels of abstraction, reanalysts had to be knowledgeable in SPSS operation. Knowledge on the use of data formats is also a requirement. Those groups that used R for analysis did not only have to show a significant mastery of R, but also of the relationship between R and SPSS Syntax and, more importantly, of the specific statistical procedures and how they are performed in both platforms. Finally, the reanalysts had to achieve a grounded understanding of the experimental design and purpose of the experiment, something that requires detailed and thoughtful study.

\subsection{Discussion and Recommendations}
Although the main focus on recomputation in HCI has focused on the replication of empirical data collection (replicated experiments), there is still much to do (and much benefit to get) from improving the recomputability of the analyses of the data gathered. In this section we discuss the main issues and lessons learned from our experience, as well as limitations from our methods and a set of recommendations on how to improve the impact and feasibility of recomputation in HCI.

\subsubsection{Reasonable Success}
Our experience shows that a group of motivated individuals achieved a modest amount of success in reanalyzing a set of simple statistical analysis of HCI empirical data. The results suggest that recomputability is within reach of the experimental HCI community, and data and analysis sharing practices will allow researchers with a stake in the correctness of other researchers' results to verify their analysis. This is possible even in the current state of affairs (many different tools being used, lack of explicit support for recomputation), but requires a significant amount of time, effort, and expertise from multiple sources. This effort and time is often not available for recomputation scenarios that require agile and fast reanalysis, such as paper article reviewing. For this, tool support and a culture change will be required. 

\subsubsection{Tools are Key (and not ready)}
SPSS might be an adequate tool for performing statistical analysis; it is successfully used by many in HCI and many other areas. Reanalysis, however, imposes a different set of constraints and requirements, and our reanalysts had many problems with the tool. Some problems relate to the general usability of the tool (which makes reanalysis difficult if you are not an SPSS expert), some to the implicit design assumption in SPSS that the data is collected, analyzed and interpreted by the same person. One of the key problems of using SPSS to enable recomputation is that there is no easy way to establish a clear correspondence between the results of the analysis in SPSS and the specific statistics extracted for the paper text, tables and graphics. 

R seems better suited for these tasks. It is possible to write R code that integrates with the text through Sweave~\cite{lmucs-papers:Leisch:2002} so that the specific analyses are compiled together with the PDF document. This makes the origin and procedure used to obtain a particular numerical result traceable to the data, and therefore easier to check and recompute. Although this is highly desirable it might still be unreasonable to demand that everyone writing or reviewing HCI and psychology papers master a programming language and tools that are generally not renowned for their usability, and that everyone is able to deal with the installation hassles of R, Sweave, ggplot2, \LaTeX, etc. in their operating system of choice. There is room for improvement for these tools, and distributing VMs may further help, but commercial tools still have an opportunity to retain their business if they provide features that adapt to the demands of easy recomputation and better support for scientific reporting. Although our experience only involved R and SPSS, the example extrapolates to other commercial tools (e.g., SAS) and open source projects (SciPy) in the statistical arena.

\subsubsection{A Culture Change}
Recomputation is therefore feasible and likely to become easier in the near future through better support and tools. However, it is unclear whether the HCI research community will embrace it. Recomputability requires more work for researchers writing papers, new habits in the analysis and reporting of experiments and, for most researchers, learning and mastering new tools. A change of culture will, however, not only mean better science through more recomputable results, but also enhanced opportunities for new analysis on old data, enabling learning from others, and making scientific fraud and bad practices easier to detect. For this all to happen, we need to start demanding from authors that they share data and analyses, and that they consider the needs of the reanalyst while planning, performing, and reporting their quantitative empirical research. A small example of this are current efforts by one of the authors to make data available to the research community through purpose-made interfaces that enable analysis and reanalysis of previous results \cite{Grijincu:2014}. 

\subsubsection{Limitations}
To our knowledge, this section reports the first study of recomputation of the statistical analysis of HCI empirical data. We have been able to learn valuable lessons from this experience, including ways to improve the actual data and analysis kit for the original study. However this only represents a semi-informal study with semi-controlled observation for one specific case analyzed using a specific tool (SPSS). Further research is required to validate these results and generalize the lessons learned to other tools and other types of reanalysts; specifically, it would be useful to investigate how experts in a particular field go about reanalysing existing results, and what are the specific barriers present when the data and analyses are prepared with a more sophisticated system such as R with Sweave and \LaTeX.

\subsubsection{Recommendations}
For the recomputability of quantitative analyses in HCI research, we make the following recommendations:
\begin{itemize}
\item When possible, share the raw data and analysis for experiments to enable recomputability.
\item Aim to reduce the knowledge required to reanalyze data. Reanalyst teams already require knowledge of the topic area, the reanalysis tool, and the computational procedures.
\item Make results explicitly traceable from computation to report.
\item After the paper is written, revise and adapt data for consistency of nomenclature of factors and condition names.
\item Due to cost, fitness and availability, favor open source tool platforms for analysis and reanalysis preparation (at least for the moment).
\item To reduce overheads due to learning of open tools by reanalysts, provide also clear instructions with the data and links to resources for learning and using the reanalysis tools.
\item To establish a replicability research culture, demand that research authors provide data and analysis at publication time.
\end{itemize}
