\section{Case study \#1: Ethical requirements for recomputation}
\label{s:group1}

Human subjects research involves the collection of data through interaction
with individuals, or through collection of personally identifiable
information. Such research poses specific barriers to reproducibility. In
fields such as human-computer interaction, there are few replications of
previous work, attributable to a culture which does not reward
reproducibility, difficulty in replicating interaction techniques when
materials are not shared, and an emphasis on formative work which proposes new
techniques over summative work~\cite{hornbaek:replications}.

In the first of our case studies, we look at one specific challenge to reproducibility in HCI
research: capturing and disseminating the ethical requirements of an
experiment, such that others may better replicate the procedures of a
study. What we wish to make possible is the following scenario: 

\renewcommand{\labelenumi}{Step \arabic{enumi}:}
\begin{enumerate}
\item Alice undertakes an experiment where she collects human data. 
\item Alice then analyses that data and publishes an academic paper.
\item To compile with recomputation standards Alice then creates a VM 
which contains the data she collected and scripts to create the statistics used in the paper 
and places the VM on recomputation.org. 
\item Bob reads Alice's research paper and is interested in comparing her work to his own.
\item Bob goes to recomputation.org and downloads the VM.
He recomputes all of Alice's experiments in order to verify the analysis in her paper.
\item Bob compares the data Alice has provided with his own and publishes his own paper.
\item To compile with recomputation standards Bob then creates a VM which contains 
the data he collected and Alice's data and scripts to create the statistics used in the paper 
then places this VM on recomputation.org
\end{enumerate}
\renewcommand{\labelenumi}{\arabic{enumi}}

We evaluate the ethics requirements procedures of ten universities to
determine a minimum specification for reporting ethical considerations, and
discuss the state of the art in standardising ethical requirements by
professional bodies and research councils.

\subsection{Background}
Human subjects research often involves the collection
of sensitive identifiable data about participants. To ensure participants are
not placed at undue risk by the conduct of an experiment, an increasingly
rigorous process of oversight of the ethical conduct of research institutions
has emerged in recent decades in many countries, particularly the US, where
institutional review boards (IRB) have been charged with reviewing all
clinical and human subjects research in line with federal regulations, with
significant penalties for institutions if ethics violations occur. Each
institution, however, has freedom to implement IRB processes as they see fit
so long as such regulations are upheld. This leads to great inconsistencies
between the expectations of institutions, and the processes researchers must
engage with in order for research protocols to be approved.

Internationally, the situation is even more variable. In the UK, research
councils mandate that ethics be considered in order to receive funding, but the
conduct of individual ethics committees is not regulated. Some countries may
not impose any requirements at all upon institutions.

Such variety in the conduct of ethical approval between institutions a
represents a significant barrier to reproducibility in HCI research. If a
researcher wishes to replicate a HCI experiment which uses human subjects
data, they will usually need to seek ethical approval from their own
institution. In our recent work, a survey of 505 papers using online social network
(OSN) data found that only 2\% of papers disclose any of the ethical
considerations of their work~\cite{hutton:reproducibility}.

As researchers do not routinely disclose the protocol which
received IRB approval, attempted replications may miss crucial details
necessary to conduct the experiment, and to provide to IRB when seeking
ethical approval, which can reduce the correctness of a replication.

With IRBs and ethics boards operating largely independently with little policy
coordination, there is no standardisation of ethics procedures.

Indeed, reproducibility has only recently been considered an important
ambition for HCI researchers, with nascent efforts including RepliCHI, which
has operated as a CHI workshop since 2013. Despite such efforts however, the
wider community has not considered these ethical challenges in detail.

Next, we assess the state of the art in ethics procedures to determine what
commonalities exist between institutional requirements. From this, we aim to
derive a minimum ``ethical specification'', encoding fundamental
methodological details to help researchers replicate procedures and ethical
details, and to make it easier to replicate applications to other IRBs, with
the ambition of such specifications being routinely attached to HCI
experiments.


\subsection{Comparison of Ethical Requirements across Universities}

To understand the state of ethics procedures between institutions, we
collected ethics applications forms for ten public universities located in the
UK, EU, USA, and Asia. A range of locations were chosen and a mixture of both large research intensive and smaller institutions to capture a range of
cultural and regulatory expectations, which we expect will manifest in
different procedures. All forms collected were derived from publicly
accessible sources, except for one supplied by the authors. This is in
itself a significant barrier to reproducibility. Without making procedures publicly 
available, there can be no external scrutiny about the appropriateness of some institution's
procedures, and makes it more difficult to derive standards.

For each form, two researchers independently coded each field, accounting for
differences in wording between forms so long as each attribute asked for the
same atomic information. Where one form requests expanded information
pertaining to a previously identified attribute, this was added as an
additional attribute. After independently coding the forms, the two
researchers discussed any discrepancies to arrive at a set of 145 unique
attributes, encompassing generic details, such as contact details of co-
investigators, methodological details, and institution-specific requirements,
often for insurance and liability purposes. Of these fields, only two were
common to all ten ethics forms --- the name of the principal investigator, and
whether informed consent was sought. This intersection was significantly
smaller than anticipated, and clearly does not constitute a useful minimum
ethical specification. It does however reveal two interesting properties. It
confirms our intuition that ethical procedures vary greatly between
institutions, while also identifying perhaps the single most important
objective of the ethics process: to ensure participants have given informed
consent to participate in an experiment.

% \begin{figure}
% \begin{minipage}{\linewidth}
% <<echo=FALSE,fig=TRUE>>=
% library(reshape)
% library(ggplot2)
% maps_data <- read.table("maps_dataframe",header=T)
% maps_melt <- melt(maps_data)
% maps_frame <- as.data.frame(table(maps_melt$Field))
% maps_frame <- maps_frame[order(-maps_frame$Freq),]
% tag_factor <- factor(maps_frame$Var1, levels=maps_frame$Var1)
% p <- ggplot(maps_frame, aes(x=tag_factor, y=Freq)) + geom_bar(stat = "identity",fill="slateblue4",colour="slateblue1") + scale_x_discrete(breaks=NULL,name="Fields") + scale_y_discrete(name="Frequency")
% print(p)
% @
% \caption{\label{p:ethicsFreq}Histogram showing frequency distribution of fields in ethics applications}
% \end{minipage}
% \end{figure}


\begin{figure}

\begin{minipage}{\linewidth}
\begin{center}
<<echo=FALSE,fig=TRUE,height=5,width=7>>=
library(reshape)
library(ggplot2)
library(plyr)
maps_data <- read.table("maps_dataframe_primary",header=T)
maps_melt <- melt(maps_data)
attach(maps_melt)
maps_table <- table(Field)
maps_table <- rev(sort(maps_table))
maps_frame <- as.data.frame(table(maps_melt$Field,maps_melt$Type))
maps_frame <- maps_frame[order(-maps_frame$Freq),]
#$ only to shut up my syntax highlighter

maps_frame_cdf <- melt(maps_frame)
maps_frame_cdf <- ddply(maps_frame_cdf, .(variable), transform, ecd=ecdf(value)(value))
maps_frame_cdf <- subset(maps_frame_cdf,value!=0)

p <- ggplot(maps_frame_cdf, aes(x=value)) + scale_x_continuous(breaks=1:10,name="Forms") + scale_y_continuous(name="Field frequency") + stat_ecdf(aes(colour=Var2)) + theme(panel.background = element_rect(fill = "transparent",colour = "black")) + scale_colour_manual(name="Field type",labels=c("High level","Sub-attribute"),values=c("#13456f", "#e02828"))
print(p)
@
\end{center}
\caption{\label{p:ethicsFreq}CDF showing the distribution of high-level and sub-attributes across the forms we examine. Half of high-level attributes only occur in one form, while no sub-attributes appear in more than seven forms.}

\end{minipage}
\end{figure}


Figure~\ref{p:ethicsFreq} shows the distribution of attributes across the ten
forms we examined. As shown in the long tail, 47.6\% of attributes appear
in only one form. Of these, 42\% are ``sub-attributes'', that is, requesting
additional information requested on another form. For example, while 60\% of
forms ask whether participants receive financial compensation, only one asks
whether co-investigators are compensated. We are most interested in the unique
``high-level attributes'' which emerge, as it is important to discern between
questions which capture institution-specific requirements, or may constitute
important issues which other IRBs ought to consider. We find instances of both
in our results. For example, while UCL are the only institution to ask whether
their own students are participants in the research (we assume for liability
reasons), surprisingly they are the only university to ask outright whether
health and safety precautions have been considered. Interestingly, only Aga Khan
University in Pakistan asks whether the study is a replication of a previous experiment.

\subsection{Proposed ethics specification}
Given the small intersection of
attributes in our study, we isolate the twenty most common attributes -- those
which occurred in six or more of the ten forms we examined. We combine any
semantically similar fields to produce the following set of 15 attributes,
presented in descending order of frequency.

\begin{itemize}
	\item Was consent sought?
	\item Was deception involved?
	\item Project title
	\item Study duration
	\item Are there risks to participants?
	\item Justify use of vulnerable participants
	\item PI contact details
	\item Funding body information
	\item Is likely to induce participant stress?
	\item Summarise research proposal/experimental methods
	\item Are supplementary documents attached? (consent forms, briefing info etc.)
	\item Are participants financially compensated?
	\item Is study clinical?
	\item Supervisor name
	\item Describe ethical issues 
\end{itemize}

This set of attributes covers a range of fundamental methodological details,
many of which can be encoded in a consistent fashion and attached as metadata
to support replications. In further work, we will demonstrate whether this set
of attributes is sufficient to capture key methodological details, which we
cannot assert from this strictly frequency-based exercise.

\subsubsection{Limitations}
This analysis is not intended as a rigorous survey
of ethics procedures internationally. The selection of institutions is
inherently biased, as we are only able to extract forms which are publicly
accessible, a barrier to reproducibility, and we have a particular emphasis on
the UK and US in this study. The intent of this exercise is not to make
statistical inferences about the state of the art in ethics review, but rather
to motivate the minimum set of attributes we recommend researchers disclose
when sharing their experimental methodologies. We also wish to raise awareness of the
importance of ethical procedures when considering reproducibility of research. 



