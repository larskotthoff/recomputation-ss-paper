\section{Case study \#1: Ethical requirements for recomputation}
\label{s:group1}

% \subsection{Introduction}
% The goal of recomputation\footnote{http://www.recomputation.org/} is to follow the recomputation manifesto\footnote{http://www.recomputation.org/blog/2013/04/12/the-recomputation-manifesto/} and make computational experiments recomputable by providing tools and a repository to store experiments in. The mission is stated as:
% \begin{quote}
% ``If we can compute your experiment now, anyone can recompute it 20 years from now''
% \end{quote}

% One of the considerations perhaps under considered in regards to recomputation to date is the ethical implications.

Human subjects research involves the collection of data through interaction
with individuals, or through collection of personally identifiable
information. Such research poses specific barriers to reproducibility. In
fields such as human-computer interaction (HCI), there are few replications of
previous work, attributable to a culture which does not reward
reproducibility, difficulty in replicating interaction techniques when
materials are not shared, and an emphasis on formative work which proposes new
techniques over summative work~\cite{hornbaek:replications}.

In this section, we look at one specific challenge to reproducibility in HCI
research: capturing and disseminating the ethical requirements of an
experiment, such that others may better replicate the procedures of a
study. We evaluate the ethics requirements procedures of ten universities to
determine a minimum specification for reporting ethical considerations, and
discuss the state of the art in standardising ethical requirements by
professional bodies and research councils.

\subsection{Background}
Human subjects research often involves the collection
of sensitive identifiable data about participants. To ensure participants are
not placed at undue risk by the conduct of an experiment, an increasingly
rigorous process of oversight of the ethical conduct of research institutions
has emerged in recent decades in many countries, particularly the US, where
institutional review boards (IRB) have been charged with reviewing all
clinical and human subjects research in line with federal regulations, with
significant penalties for institutions if ethics violations occur. Each
institution, however, has freedom to implement IRB processes as they see fit
so long as such regulations are upheld. This leads to great inconsitencies
between the expectations of institutions, and the processes researchers must
engage with in order for research protocols to be approved.

Internationally, the situation is even more variable. Many countries may not
impose any such requirements on institutions, while in the UK, research
councils mandate ethics are considered in order to receive funding, however
the conduct of individual ethics committees is not regulated.

% we expect processes vary between institutions due to myriad factors  Such
variety in the conduct of ethical approval between institutions a represents a
significant barrier to reproducibility in HCI research. If a researcher wishes
to replicate a HCI experiment which uses human subjects data, they will
usually need to seek ethical approval from their own institution. In our work,
a survey of 505 papers using online social network (OSN) data found that only
2\% of papers disclose any of the ethical considerations of their work.
% cite osn survey!
As researchers do not routinely disclose the protocol which
received IRB approval, attempted replications may miss crucial details
necessary to conduct the experiment, and to provide to IRB when seeking
ethical approval, which can reduce the correctness of a replication.

% no standards. replication community in hci is nascent and not considering ethics issues
% including at workshops like replichi

With IRBs and ethics boards operating largely independently with little policy
coordination, there is no standardisation of ethics procedures.

Indeed, reproducibility has only recently been considered an important
ambition for HCI researchers, with nascent efforts including RepliCHI, which
has operated as a CHI workshop since 2013. Despite such efforts however, the
wider community has not considered these ethical challenges in detail.

% we examine the state of the art to identify whether a base standard can be used to
% encourage sharing of ethics procedures
Next, we assess the state of the art in ethics procedures to determine what commonalities exist between institutional requirements. From this, we aim to derive a minimum ``ethical specification'', encoding fundamental methodological details to help researchers replicate procedures and ethical details, and to make it easier to replicate applications to other IRBs, with the ambition of such specifications being routinely attached to HCI experiments.



\subsection{Comparison of Ethical Requirements across Universities}

To understand the state of ethics procedures between institutions, we
collected ethics applications forms for ten public universities located in the
UK, EU, and USA. A range of locations were chosen to represent procedures
where cultural and regulatory expectations will likely vary.


In order to compare each ethical approval form from each institution, two
researchers independently transcribed each field and noted unique occurences,
mapping minor variations in wording to each field. Where a field potentially
encompasses additional information, unique requests for additional information
were counted as additional fields. For example, requesting information about
grants were considered one field, while providing explicit grant codes and
whether grant funding was already agreed were considered two further fields.
Our analysis uncovered 145 unique fields, encompassing generic details, such
as contact details of co-investigators, methodological details, and
institution-specific requirements, often for insurance and liability
purposes. Agreement was sought on all terms between researchers before
producing the final set. Of these fields, only two were common
to all ten ethics forms - the name of the principal investigator, and whether
informed consent was sought. While this is clearly an insufficient  minimum
specification for capturing ethics requirements, it reveals the crux of the
ethics approval process is to ensure participant agency is preserved through
the experimental process
by mandating informed consent.


% Gaterhing forms

% Why did we select each institition?

% Comment on openess? And that only open forms can be found/transcribed

% Gathering of unique fields






\subsubsection{Comparison Analysis}

\begin{figure}
\begin{minipage}{\linewidth}
<<echo=FALSE,fig=TRUE>>=
library(reshape)
library(ggplot2)
maps_data <- read.table("maps_dataframe",header=T)
maps_melt <- melt(maps_data)
maps_frame <- as.data.frame(table(maps_melt$Field))
maps_frame <- maps_frame[order(-maps_frame$Freq),]
tag_factor <- factor(maps_frame$Var1, levels=maps_frame$Var1)
p <- ggplot(maps_frame, aes(x=tag_factor, y=Freq)) + geom_bar(stat = "identity",fill="slateblue4",colour="slateblue1") + scale_x_discrete(breaks=NULL,name="Fields") + scale_y_discrete(name="Frequency")
print(p)
@
\caption{Histogram showing frequency distribution of fields in ethics applications}
\end{minipage}
\end{figure}

% comment on how we compared each paper and a report of the comparison






% which fields occur frequently

% thematic overlaps

% surprisingly small common set

% what are the outliers - are they important?


\subsection{Comparison of Ethical Requirements across Publishers}

\subsection{Framework for Ethics to accompany data and experiments}

