@Misc{summerschoolpaper,
    year = 2014,
    url = "https://github.com/larskotthoff/recomputation-ss-paper/",
    key = {GitHub},
    title = "{GitHub} repository for Recomputation Summer School Paper"}

@Misc{executablepaper,
    year = 2011,
    url = "http://www.elsevier.com/physical-sciences/computer-science/executable-papers",
    author = "Elsevier",
    title = "Executable Papers Grand Challenge"}

    
@Misc{parasols,
    year = 2014,
    url = "https://github.com/ciaranm/parasols/",
    author = "Ciaran McCreesh",
    title = "{GitHub} repository for Parallel Solvers for Hard Problems",
}

@Misc{trole,
    year = 2014,
    url = {https://github.com/mbareford/trole},
    author = {Michael Bareford},
    title = {{GitHub} repository for {Taylor} Relaxation of Loop
        Ensembles},
}

@Misc{gsl,
    title = {{GSL} -- {GNU Scientific Library}},
    key = {gsl},
    howpublished = {\url{http://www.gnu.org/software/gsl/}},
    note = {Accessed 1 September 2014},
}

@Misc{nag,
    title = {The {NAG} {C} Library},
    key = {nag},
    howpublished = {\url{http://www.nag.co.uk/numeric/CL/CLdescription.asp}},
    note = {Accessed 1 September 2014},
}

@misc{ssj,
    title = {{SSJ}: Stochastic Simulation in {Java}},
    key = {ssj},
    howpublished = {\url{http://simul.iro.umontreal.ca/ssj-2/indexe.html}},
    note = {Accessed 1 September 2014},
}

@misc{quanticol,
    title = {{QUANTICOL}: A Quantitative Approach to Management and
        Design of Collective and Adaptive Behaviours},
    key = {quanticol},
    howpublished = {\url{http://blog.inf.ed.ac.uk/quanticol/}},
    note = {Accessed 1 September 2014},
}

@misc{dimacs,
    title = {{DIMACS} Implementation Challenges},
    author = {{DIMACS}},
    howpublished = {\url{http://dimacs.rutgers.edu/Challenges/}},
    note = {Accessed 1 September 2014},
}

@Misc{gent:recomputation,
  author      =	{Gent, Ian P.},
  title	      =	{The Recomputation Manifesto},
  abstract    =	{Replication of scientific experiments is critical to the advance of science. Unfortunately, the discipline of Computer Science has never treated replication seriously, even though computers are very good at doing the same thing over and over again. Not only are experiments rarely replicated, they are rarely even replicable in a meaningful way. Scientists are being encouraged to make their source code available, but this is only a small step. Even in the happy event that source code can be built and run successfully, running code is a long way away from being able to replicate the experiment that code was used for. I propose that the discipline of Computer Science must embrace replication of experiments as standard practice. I propose that the only credible technique to make experiments truly replicable is to provide copies of virtual machines in which the experiments are validated to run. I propose that tools and repositories should be made available to make this
		happen. I propose to be one of those who makes it happen.},
  archiveprefix={arXiv},
  day	      =	{12},
  eprint      =	{1304.3674},
  keywords    =	{recomputation, reproducibility, research, virtualisation},
  month	      =	apr,
  arxiv =	{1304.3674},
  year	      =	{2013}
}

@Misc{ciaran,
  Author = {McCreesh, Ciaran and Prosser, Patrick},
	Title = {The Shape of the Search Tree for the Maximum Clique Problem, and the Implications for Parallel Branch and Bound},
  Year = {2014},
	arxiv = {1401.5921}}
  
@Misc{azure,
	howpublished = {\url{https://azure.microsoft.com/en-us/}},
	Title = {{Microsoft Azure}},
	key = {{Microsoft Azure}},
    note = {Accessed 31 August 2014},
}

@inproceedings{chord,
author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M. Frans and Balakrishnan, Hari},
 title = {Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications},
 booktitle = {Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications},
 year = {2001},
 isbn = {1-58113-411-8},
 location = {San Diego, CA, USA},
 pages = {149--160},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/383059.383071},
 doi = {10.1145/383059.383071},
 acmid = {383071},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{danielpaper,
  Author = {Reijsbergen, Dani\"el and Gilmore, Stephen},
	Title = {Formal punctuality analysis of frequent bus
services using headway data},
booktitle={Proceedings of the Eleventh European Workshop on Performance Engineering (EPEW 2014)},
  Year = {2014},
  month = sep,
	}
	
	
@article{bareford2010nanoflare,
  title={A nanoflare distribution generated by repeated relaxations triggered by kink instability},
  author={Bareford, M and Browning, P and Van der Linden, R},
  doi={10.1051/0004-6361/201014067},
  journal={Astronomy and Astrophysics},
  number = {A70},
  volume={521},
  month=oct,
  year={2010},
  publisher={EDP Sciences}
}

@article{bareford2011flare,
  title={The flare-energy distributions generated by kink-unstable ensembles of zero-net-current coronal loops},
  author={Bareford, M and Browning, P and Van der Linden, R},
  journal={Solar Physics},
  volume={273},
  number={1},
  pages={93--115},
  year={2011},
  publisher={Springer}
}

@misc{arabas2013libcloud,
  title={libcloudph{++} 0.1: single-moment bulk, double-moment bulk, and particle-based warm-rain microphysics library in {C++}},
  author={Arabas, Sylwester and Jaruga, Anna and Pawlowska, Hanna and Grabowski, Wojciech W},
  arxiv = {1310.1905},
  year={2013}
}

@inproceedings{hornbaek:replications,
    abstract = {{A replication is an attempt to confirm an earlier study's findings. It is often claimed that research in Human-Computer Interaction (HCI) contains too few replications. To investigate this claim we examined four publication outlets (891 papers) and found 3\% attempting replication of an earlier result. The replications typically confirmed earlier findings, but treated replication as a confirm/not-confirm decision, rarely analyzing effect sizes or comparing in depth to the replicated paper. When asked, most authors agreed that their studies were replications, but rarely planned them as such. Many non-replication studies could have corroborated earlier work if they had analyzed data differently or used minimal effort to collect extra data. We discuss what these results mean to HCI, including how reporting of studies could be improved and how conferences/journals may change author instructions to get more replications.}},
    address = {New York, NY, USA},
    author = {Hornbaek, Kasper and Sander, S{\o}ren S. and Javier Andr\'{e}s Bargas Avila and Simonsen, Jakob G.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    citeulike-article-id = {13160301},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2557004},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/2556288.2557004},
    doi = {10.1145/2556288.2557004},
    isbn = {978-1-4503-2473-1},
    keywords = {hci, repro},
    location = {Toronto, ON, Canada},
    pages = {3523--3532},
    posted-at = {2014-05-06 14:18:35},
    priority = {2},
    publisher = {ACM},
    series = {CHI '14},
    title = {{Is Once Enough?: On the Extent and Content of Replications in Human-computer Interaction}},
    url = {http://dx.doi.org/10.1145/2556288.2557004},
    year = {2014}
  }

@article{kotthoff,
     author = {Kotthoff, Lars},
     title = {Reliability of computational experiments on virtualised hardware},
     journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
     volume = {26},
     number = {1},
     pages = {33-49},
     year = {2014},
     doi = {10.1080/0952813X.2013.784812},
     URL = {http://dx.doi.org/10.1080/0952813X.2013.784812 },
     eprint = {http://dx.doi.org/10.1080/0952813X.2013.784812}
}

@article{GMD_editorial_2013,
  title={Editorial: The publication of geoscientific model developments v1.0},
  author={{GMD Executive Editors}},
  journal={Geoscientific Model Development},
  doi = {10.5194/gmd-6-1233-2013},
  volume={6},
  number={4},
  pages={1233--1242},
  year={2013}
}


@InProceedings{drummond:replicability,
  author      =	{Drummond, Chris},
  title	      =	{Replicability is not Reproducibility: Nor is it Good Science},
  booktitle   =	{Proceedings of the Evaluation Methods for Machine Learning Workshop at the 26th ICML},
  keywords    =	{machine-learning, reproducibility, reproducible-research},
  location    =	{Montreal, QC, Canada},
  url	      =	{http://cogprints.org/7691/},
  year	      =	{2009}
}

@Article{stodden:practices,
  author      =	{Stodden, Victoria and Miguez, Sheila},
  title	      =	{Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research},
  day	      =	{09},
  doi	      =	{10.5334/jors.ay},
  issn	      =	{2049-9647},
  journal     =	{Journal of Open Research Software},
  keywords    =	{computational-science, for:lukehutton, reproducibility, reproducible-research},
  month	      =	jul,
  number      =	{1},
  pages	      =	{21+},
  url	      =	{http://dx.doi.org/10.5334/jors.ay},
  volume      =	{2},
  year	      =	{2014}
}

@Article{mesirov:accessible,
  author      =	{Mesirov, Jill P.},
  title	      =	{Accessible Reproducible Research},
  abstract    =	{10.1126/science.1179653},
  day	      =	{22},
  doi	      =	{10.1126/science.1179653},
  issn	      =	{1095-9203},
  journal     =	{Science},
  keywords    =	{reproducibility, reproducible-research},
  month	      =	jan,
  number      =	{5964},
  pages	      =	{415--416},
  pmid	      =	{20093459},
  publisher   =	{American Association for the Advancement of Science},
  url	      =	{http://dx.doi.org/10.1126/science.1179653},
  volume      =	{327},
  year	      =	{2010}
}

@Article{johnson:evidence,
  author      =	{Johnson, Valen E.},
  title	      =	{Revised standards for statistical evidence.},
  abstract    =	{Recent advances in Bayesian hypothesis testing have led to the development of uniformly most powerful Bayesian tests, which represent an objective, default class of Bayesian hypothesis tests that have the same rejection regions as classical significance tests. Based on the correspondence between these two classes of tests, it is possible to equate the size of classical hypothesis tests with evidence thresholds in Bayesian tests, and to equate P values with Bayes factors. An examination of these connections suggest that recent concerns over the lack of reproducibility of scientific studies can be attributed largely to the conduct of significance tests at unjustifiably high levels of significance. To correct this problem, evidence thresholds required for the declaration of a significant finding should be increased to 25-50:1, and to 100-200:1 for the declaration of a highly significant finding. In terms of classical hypothesis tests, these evidence standards mandate the
		conduct of tests at the 0.005 or 0.001 level of significance.},
  day	      =	{26},
  doi	      =	{10.1073/pnas.1313476110},
  issn	      =	{1091-6490},
  journal     =	{Proceedings of the National Academy of Sciences of the United States of America},
  keywords    =	{research, significance, statistics},
  month	      =	nov,
  number      =	{48},
  pages	      =	{19313--19317},
  pmcid	      =	{PMC3845140},
  pmid	      =	{24218581},
  publisher   =	{National Academy of Sciences},
  url	      =	{http://dx.doi.org/10.1073/pnas.1313476110},
  volume      =	{110},
  year	      =	{2013}
}

@Article{davison:reproducibility,
  author      =	{Davison, Andrew},
  title	      =	{Automated capture of experiment context for easier reproducibility in computational research},
  abstract    =	{Reproducibility is part of the definition of the scientific method. In computational science, reproducibility has practical benefits in allowing code reuse and hence enabling more rapid progress and the ability to develop more complex models and analysis methods. However, it is widely recognized that published research in diverse scientific domains that rely on numerical computation is too infrequently reproducible, with some commentators speaking of a ``credibility crisis''. In this article, I examine the reasons why reproducibility is difficult to achieve, argue that for computational research to become consistently and reliably reproducible requires that reproducibility become \emph{easy} to achieve, as part of day-to-day research and not just for that subset of research that is published, and suggest a combination of best practices and automated tools that can make reproducible research easier.},
  address     =	{Los Alamitos, CA, USA},
  doi	      =	{10.1109/mcse.2012.41},
  issn	      =	{1521-9615},
  journal     =	{Computing in Science and Engineering},
  keywords    =	{context, experiments, for:lukehutton, reproducibility, research, workflow},
  month       = jul,
  number      =	{4},
  pages       = {48--56},
  publisher   =	{IEEE Computer Society},
  url	      =	{http://dx.doi.org/10.1109/mcse.2012.41},
  volume      =	{14},
  year	      =	{2012}
}

@Misc{hutton:reproducibility,
    author = {Luke Hutton and Tristan Henderson},
    title = {Towards reproducibility in online social network 
        research},
    note = {In submission to this same special issue},
    month = aug,
    year = 2014,
}

@Article{donoho:reproducible,
  author      =	{Donoho, David L. and Maleki, Arian and Rahman, Inam U. and Shahram, Morteza and Stodden, Victoria},
  title	      =	{Reproducible Research in Computational Harmonic Analysis},
  abstract    =	{Scientific computation is emerging as absolutely central to the scientific method. Unfortunately, it's error-prone and currently immatureatraditional scientific publication is incapable of finding and rooting out errors in scientific computationawhich must be recognized as a crisis. An important recent development and a necessary response to the crisis is reproducible computational research in which researchers publish the article along with the full computational environment that produces the results. The authors have practiced reproducible computational research for 15 years and have integrated it with their scientific research and with doctoral and postdoctoral education. In this article, they review their approach and how it has evolved over time, discussing the arguments for and against working reproducibly.},
  address     =	{Los Alamitos, CA, USA},
  doi	      =	{10.1109/mcse.2009.15},
  issn	      =	{1521-9615},
  journal     =	{Computing in Science \& Engineering},
  keywords    =	{computational-science, reproducibility},
  month	      =	jan,
  number      =	{1},
  pages	      =	{8--18},
  publisher   =	{IEEE Computer Society},
  url	      =	{http://dx.doi.org/10.1109/mcse.2009.15},
  volume      =	{11},
  year	      =	{2009}
}

@Article{peng:reproducible,
  author      =	{Peng, Roger D.},
  title	      =	{Reproducible Research in Computational Science},
  abstract    =	{Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  day	      =	{02},
  doi	      =	{10.1126/science.1213847},
  issn	      =	{1095-9203},
  journal     =	{Science},
  keywords    =	{reproducibility, research, science},
  month	      =	dec,
  number      =	{6060},
  pages	      =	{1226--1227},
  pmcid	      =	{PMC3383002},
  pmid	      =	{22144613},
  publisher   =	{American Association for the Advancement of Science},
  url	      =	{http://dx.doi.org/10.1126/science.1213847},
  volume      =	{334},
  year	      =	{2011}
}

@Article{bonnet:repeatability,
  author      =	{Bonnet, Philippe and Manegold, Stefan and Bj{\o}rling, Matias and Cao, Wei and Gonzalez, Javier and Granados, Joel and Hall, Nancy and Idreos, Stratos and Ivanova, Milena and Johnson, Ryan and Koop, David and Kraska, Tim and M\"{u}ller, Ren\'{e} and Olteanu, Dan and Papotti, Paolo and Reilly, Christine and Tsirogiannis, Dimitris and Yu, Cong and Freire, Juliana and Shasha, Dennis},
  title	      =	{Repeatability and Workability Evaluation of {SIGMOD} 2011},
  abstract    =	{SIGMOD has offered, since 2008, to verify the experiments published in the papers accepted at the conference. This year, we have been in charge of reproducing the experiments provided by the authors (repeatability), and exploring changes to experiment parameters (workability). In this paper, we assess the SIGMOD repeatability process in terms of participation, review process and results. While the participation is stable in terms of number of submissions, we find this year a sharp contrast between the high participation from Asian authors and the low participation from American authors. We also find that most experiments are distributed as Linux packages accompanied by instructions on how to setup and run the experiments. We are still far from the vision of executable papers.},
  address     =	{New York, NY, USA},
  doi	      =	{10.1145/2034863.2034873},
  issn	      =	{0163-5808},
  journal     =	{ACM SIGMOD Record},
  keywords    =	{reproducibility},
  month	      =	sep,
  number      =	{2},
  pages	      =	{45--48},
  publisher   =	{ACM},
  url	      =	{http://dx.doi.org/10.1145/2034863.2034873},
  volume      =	{40},
  year	      =	{2011}
}

@Article{ioannidis:repeatability,
  author      =	{Ioannidis, John P. A. and Allison, David B. and Ball, Catherine A. and Coulibaly, Issa and Cui, Xiangqin and Culhane, Aedin C. and Falchi, Mario and Furlanello, Cesare and Game, Laurence and Jurman, Giuseppe and Mangion, Jon and Mehta, Tapan and Nitzberg, Michael and Page, Grier P. and Petretto, Enrico and van Noort, Vera},
  title	      =	{Repeatability of published microarray gene expression analyses},
  abstract    =	{Given the complexity of microarray-based gene expression studies, guidelines encourage transparent design and public data availability. Several journals require public data deposition and several public databases exist. However, not all data are publicly available, and even when available, it is unknown whether the published results are reproducible by independent scientists. Here we evaluated the replication of data analyses in 18 articles on microarray-based gene expression profiling published in Nature Genetics in 2005a2006. One table or figure from each article was independently evaluated by two teams of analysts. We reproduced two analyses in principle and six partially or with some discrepancies; ten could not be reproduced. The main reason for failure to reproduce was data unavailability, and discrepancies were mostly due to incomplete data annotation or specification of data processing and analysis. Repeatability of published microarray studies is apparently
		limited. More strict publication rules enforcing public data availability and explicit description of data processing and analysis should be considered.},
  day	      =	{28},
  doi	      =	{10.1038/ng.295},
  issn	      =	{1061-4036},
  journal     =	{Nature Genetics},
  keywords    =	{microarray, reproducibility},
  month	      =	feb,
  number      =	{2},
  pages	      =	{149--155},
  pmid	      =	{19174838},
  publisher   =	{Nature Publishing Group},
  url	      =	{http://dx.doi.org/10.1038/ng.295},
  volume      =	{41},
  year	      =	{2009}
}


@Article{howe:reproducible,
  author      =	{Howe, Bill},
  title	      =	{Virtual Appliances, Cloud Computing, and Reproducible Research},
  abstract    =	{As science becomes increasingly computational, reproducibility has become increasingly difficult, perhaps surprisingly. In many contexts, virtualization and cloud computing can mitigate the issues involved without significant overhead to the researcher, enabling the next generation of rigorous and reproducible computational science.},
  doi	      =	{10.1109/mcse.2012.62},
  issn	      =	{1521-9615},
  journal     =	{Computing in Science \& Engineering},
  keywords    =	{cloud-computing, reproducibility, research},
  month	      =	jul,
  number      =	{4},
  pages	      =	{36--41},
  url	      =	{http://dx.doi.org/10.1109/mcse.2012.62},
  volume      =	{14},
  year	      =	{2012}
}

@Misc{pineda-krch:sweave,
    author = {Pineda-Krch, Mario},
    title = {The Joy of {Sweave}: A Beginner's Guide to Reproducible
        Research with {Sweave}},
    year = 2011,
    month = jan,
    day = 17,
    url =
    {http://www.mathstat.ualberta.ca/~mlewis/links/the_joy_of_sweave_v1.pdf},
}

@Article{baker:verify,
  author      =	{Baker, Monya},
  title	      =	{Independent labs to verify high-profile papers},
  day	      =	{14},
  doi	      =	{10.1038/nature.2012.11176},
  issn	      =	{1476-4687},
  journal     =	{Nature},
  keywords    =	{reproducibility, research, science, nopdf},
  month	      =	aug,
  url	      =	{http://dx.doi.org/10.1038/nature.2012.11176},
  year	      =	{2012}
}

@Misc{github:citable,
    author = {GitHub},
    title = {Making Your Code Citable},
    howpublished = {\url{https://guides.github.com/activities/citable-code/}},
    year = 2014,
    month = may,
    note = {Accessed 28 August 2014},
}

@Misc{figshare:citable,
    author = {FigShare},
    title = {All research outputs should be citable},
    howpublished = {\url{http://figshare.com/blog/All%20research%20outputs%20should%20be%20citable/32}},
    year = 2012,
    month = jun,
    day = 12,
    note = {Accessed 28 August 2014},
}

@inproceedings{Nacenta:memorability,
    author    = {Miguel A. Nacenta and Yemliha Kamber and Yizhou Qiang and Per Ola Kristensson},
    title     = {Memorability of pre-designed and user-defined gesture sets},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    year      = {2013},
    pages     = {1099-1108},
    location = {Paris, France},
    month = apr,
    doi = {10.1145/2470654.2466142},
}

@Misc{NSF,
    title = {A vision and strategy for software for science, engineering and education},
      url = {http://www.nsf.gov/pubs/2012/nsf12113/nsf12113.pdf},
   author = {NSF},  
     year = {2012},
   publisher = {US National Science Foundation}
  }

@inproceedings{lmucs-papers:Leisch:2002,
      author = {Friedrich Leisch},
        title = {Sweave: Dynamic Generation of Statistical Reports Using
                      Literate Data Analysis},
          booktitle = {Compstat 2002 --- Proceedings in Computational
                      Statistics},
            pages = {575--580},
              year = 2002,
                editor = {Wolfgang H{\"a}rdle and Bernd R{\"o}nz},
                  publisher = {Physica Verlag, Heidelberg},
                    note = {ISBN 3-7908-1517-9},
                      url =
                      {http://www.stat.uni-muenchen.de/~leisch/Sweave},
                      doi = {10.1007/978-3-642-57489-4_89},
}

@article{Merali_2010,
     title = {Computational science: Error, why scientific programming does not compute},
    author = {Merali, Zeeya},
   journal = {Nature},
    volume = {467},
    number = {7317},
     pages = {775--777},
      year = {2010},
       doi = {10.1038/467775a}
}

@Misc{emcsr_arxiv_draft,
  author      =	{Arabas, Sylwester and Bareford, Michael R. and Gent, Ian P. and Gorman, Benjamin M. and Hajiarabderkani, Masih and Henderson, Tristan and Hutton, Luke and Konovalov, Alexander and Kotthoff, Lars and McCreesh, Ciaran and Paul, Ruma R. and Petrie, Karen E. J. and Razaq, Abdul and Reijsbergen, Dani\"{e}l},
  title	      =	{An Open and Reproducible Paper on Openness and Reproducibility of Papers in Computational Science},
  abstract    =	{This paper was written by participants of the First Summer School on Experimental Methodology in Computational Science Research. We report several case studies performed during the school and systematize our experiences. This paper is open and reproducible: the whole process of writing this paper is captured in the GitHub repository hosting both the source of the paper, supplementary codes and data; we are providing setup for several experiments on which we were working; finally, we are trying to describe what we have achieved during the week of the school in a way that others may repeat (and hopefully improve) our experiments.},
  archiveprefix={arXiv},
  day	      =	{9},
  eprint      =	{1408.2123},
  keywords    =	{crawdad_mention, reproducibility, emcsr2014},
  month	      =	aug,
  arxiv       = {1408.2123},
  year	      =	{2014}
}
@article{shrdlu,
      title={Understanding natural language},
        author={Winograd, Terry},
          journal={Cognitive Psychology},
            volume={3},
              number={1},
                pages={1--191},
                  year={1972},
                    publisher={Elsevier}
}

@article{drummond09,
    title = "Replicability is not reproducibility: Nor is it good science",
    author = "Chris Drummond",
    year = "2009",
    journal =  "Proceedings of the Twenty-Sixth International Conference on Machine Learning: Workshop on Evaluation Methods for Machine Learning IV"}

    @article{hownot,
          title={How not to do it},
            author={Gent, I.P. and Grant, S.A. and MacIntyre, E. and Prosser, P. and Shaw, P. and Smith, B.M. and Walsh, T.},
              journal={Research Report Series-University of Leeds School of Computer studies LU SCS RR},
                year={1997},
                  publisher={UNIVERSITY OF LEEDS}
    }


@article{DBLP:journals/cse/GavishD12,
  author    = {Matan Gavish and
               David L. Donoho},
  title     = {Three Dream Applications of Verifiable Computational Results},
  journal   = {Computing in Science and Engineering},
  volume    = {14},
  number    = {4},
  year      = {2012},
  pages     = {26-31},
  ee        = {http://doi.ieeecomputersociety.org/10.1109/MCSE.2012.65},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@inproceedings{sumatra-davison,
    author = "Andrew Davison",
    title = "Automated tracking of computational experiments using Sumatra",
    howpublished = "\url{http://www.andrewdavison.info/media/slides/sumatra_euroscipy2010.pdf}",
    booktitle = "EuroSciPy: 3rd European meeting on Python in Science",
    year = 2010}

@inproceedings{DBLP:conf/eScience/StoddenHP12,
  author    = {Victoria Stodden and
               Christophe Hurlin and
               Christophe Perignon},
  title     = {RunMyCode.org: A novel dissemination and collaboration platform
               for executing published computational results},
  booktitle = {eScience},
  year      = {2012},
  pages     = {1-8},
  ee        = {http://doi.ieeecomputersociety.org/10.1109/eScience.2012.6404455},
  crossref  = {DBLP:conf/eScience/2012},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}
@proceedings{DBLP:conf/eScience/2012,
  title     = {8th IEEE International Conference on E-Science, e-Science
               2012, Chicago, IL, USA, October 8-12, 2012},
  booktitle = {e-Science},
  publisher = {IEEE Computer Society},
  year      = {2012},
  isbn      = {978-1-4673-4467-8},
  ee        = {http://ieeexplore.ieee.org/xpl/mostRecentIssue.jsp?punumber=6389598},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{DBLP:journals/fgcs/RoureGS09,
  author    = {David De Roure and
               Carole A. Goble and
               Robert Stevens},
  title     = {The design and realisation of the my$_{\mbox{Experiment}}$
               Virtual Research Environment for social sharing of workflows},
  journal   = {Future Generation Comp. Syst.},
  volume    = {25},
  number    = {5},
  year      = {2009},
  pages     = {561-567},
  ee        = {http://dx.doi.org/10.1016/j.future.2008.06.010},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}


@book{DBLP:books/sp/Schulte02,
  author    = {Christian Schulte},
  title     = {Programming Constraint Services: High-Level Programming
               of Standard and New Constraint Services},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {2302},
  year      = {2002},
  isbn      = {3-540-43371-6},
  ee        = {http://dx.doi.org/10.1007/3-540-45945-6},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{worthdoingtwice,
    title = "If a job is worth doing, it is worth doing twice",
    journal = "Nature", 
    year = 2013,
    pages = "7", 
    volume = 496,
    author = "Jonathan F. Russell",
    month = "April",
    doi = "doi:10.1038/496007a",
    ee = "http://www.nature.com/news/if-a-job-is-worth-doing-it-is-worth-doing-twice-1.12727"
}

@inproceedings{DBLP:conf/lion/NellFHL11,
  author    = {Christopher Nell and
               Chris Fawcett and
               Holger H. Hoos and
               Kevin Leyton-Brown},
  title     = {HAL: A Framework for the Automated Analysis and Design of
               High-Performance Algorithms},
  booktitle = {LION},
  year      = {2011},
  pages     = {600-615},
  ee        = {http://dx.doi.org/10.1007/978-3-642-25566-3_47},
  crossref  = {DBLP:conf/lion/2011},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}
@proceedings{DBLP:conf/lion/2011,
  editor    = {Carlos A. Coello Coello},
  title     = {Learning and Intelligent Optimization - 5th International
               Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected
               Papers},
  booktitle = {LION},
  publisher = {Springer},
  series    = {Lecture Notes in Computer Science},
  volume    = {6683},
  year      = {2011},
  isbn      = {978-3-642-25565-6},
  ee        = {http://dx.doi.org/10.1007/978-3-642-25566-3},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}



@Article{InceD2012Case,
  author =       {Darrel C. Ince and Leslie Hatton and John Graham-Cumming}, 
  title =        {The case for open computer programs},
  journal =      {Nature},
  year =         {2012},
  volume =       {482},
  number =       {7386},
  pages =        {485-488},
  month =        {february},
  DOI =          {10.1038/nature10836},
  PMID =         {22358837},
  WOBIB =        {},
  OPTnote =      {},
  OPTannote =    {}
}

@article{DBLP:journals/sosym/GorpG12,
  author    = {Pieter Van Gorp and
               Paul W. P. J. Grefen},
  title     = {Supporting the internet-based evaluation of research software
               with cloud infrastructure},
  journal   = {Software and System Modeling},
  volume    = {11},
  number    = {1},
  year      = {2012},
  pages     = {11-28},
  ee        = {http://dx.doi.org/10.1007/s10270-010-0163-y},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}


@misc{shrdlu-resurrection,
      title="{SHRDLU} resurrection",
      year = 2011,
      author = "Semaphore~Corporation",
        howpublished = "\url{http://www.semaphorecorp.com/misc/shrdlu.html}"
}

@misc{claerbout,
      author = "Jon Claerbout",
      title = "REPRODUCIBLE COMPUTATIONAL RESEARCH: A history of hurdles, mostly overcome",
        howpublished = "\url{http://sepwww.stanford.edu/sep/jon/reproducible.html}"
}

@misc{brown,
      author = "C. Titus  Brown",
      title = "Our approach to replication in computational science",
        howpublished = "\url{http://ivory.idyll.org/blog/replication-i.html}",
        month = apr,
        day = 2,
            year = 2012
}

@misc{guo-phd,
    author = "Philip J. Guo",
   title = "Software Tools to Facilitate Research Programming",
       year = 2012,
   howpublished = "Ph.D. dissertation, Department of Computer Science, Stanford University"}

@inproceedings{DBLP:conf/csclp/BeckPW04,
      author    = {J. Christopher Beck and
                         Patrick Prosser and
                                            Richard J. Wallace},
        title     = {Trying Again to Fail-First},
          booktitle = {CSCLP},
            year      = {2004},
              pages     = {41-55},
                ee        = {http://dx.doi.org/10.1007/11402763_4},
                  crossref  = {DBLP:conf/csclp/2004},
                    bibsource = {DBLP, http://dblp.uni-trier.de}
}

@proceedings{DBLP:conf/csclp/2004,
      editor    = {Boi Faltings and
                         Adrian Petcu and
                                            Fran\c{c}ois Fages and
                                                           Francesca Rossi},
        title     = {Recent Advances in Constraints, Joint ERCIM/CoLogNet International
                           Workshop on Constraint Solving and Constraint Logic Programming,
                                          CSCLP 2004, Lausanne, Switzerland, June 23-25, 2004, Revised
                                                             Selected and Invited Papers},
          booktitle = {CSCLP},
            publisher = {Springer},
              series    = {Lecture Notes in Computer Science},
                volume    = {3419},
                  year      = {2005},
                    isbn      = {3-540-25176-6},
                      bibsource = {DBLP, http://dblp.uni-trier.de}
}



@Misc{Brown2012,
      abstract = {Deep shotgun sequencing and analysis of genomes, transcriptomes, amplified single-cell genomes, and metagenomes has enabled investigation of a wide range of organisms and ecosystems. However, sampling variation in short-read data sets and high sequencing error rates of modern sequencers present many new computational challenges in data interpretation. These challenges have led to the development of new classes of mapping tools and \{$\backslash$em de novo\} assemblers. These algorithms are challenged by the continued improvement in sequencing throughput. We here describe digital normalization, a single-pass computational algorithm that systematizes coverage in shotgun sequencing data sets, thereby decreasing sampling variation, discarding redundant data, and removing the majority of errors. Digital normalization substantially reduces the size of shotgun data sets and decreases the memory and time requirements for \{$\backslash$em de novo\} sequence assembly, all without significantly impacting content of the generated contigs. We apply digital normalization to the assembly of microbial genomic data, amplified single-cell genomic data, and transcriptomic data. Our implementation is freely available for use and modification.},
        added-at = {2012-11-08T12:19:32.000+0100},
          author = {Brown, C Titus and Howe, Adina and Zhang, Qingpeng and Pyrkosz, Alexis B and Brom, Timothy H},
            biburl = {http://www.bibsonomy.org/bibtex/21fb5a230c0946fc42e2737a2eda7c528/jabreftest},
              file = {Brown2012_01.pdf:Brown2012_01.pdf:PDF;Brown2012_00.txt:Brown2012_00.txt:Text},
                groups = {public},
                  interhash = {ab786d8050c05f62d84cc7a7b5e94eb2},
                    intrahash = {1fb5a230c0946fc42e2737a2eda7c528},
                      keywords = {normalization rna-seq},
                        timestamp = {2012-11-08T12:19:32.000+0100},
                          title = {{A Reference-Free Algorithm for Computational Normalization of Shotgun Sequencing Data}},
                            arxiv = {1203.4802},
                              username = {jabreftest},
                                year = 2012
}


@article{DBLP:journals/algorithms/Prosser12,
      author    = {Patrick Prosser},
        title     = {Exact Algorithms for Maximum Clique: A Computational Study},
          journal   = {Algorithms},
            volume    = {5},
              number    = {4},
                year      = {2012},
                  pages     = {545-587},
                    ee        = {http://dx.doi.org/10.3390/a5040545},
                      bibsource = {DBLP, http://dblp.uni-trier.de}
}


@article{10.1109/MCSE.2012.62,
    author = {Bill Howe},
    title = {Virtual Appliances, Cloud Computing, and Reproducible Research},
    journal ={Computing in Science and Engineering},
    volume = {14},
    number = {4},
    issn = {1521-9615},
    year = {2012},
    pages = {36-41},
    doi = {http://doi.ieeecomputersociety.org/10.1109/MCSE.2012.62},
    publisher = {IEEE Computer Society},
    address = {Los Alamitos, CA, USA},
}

@Misc{mozillasciencelabs,
    title = {{Mozilla Science Labs}},
    key = {{Mozilla Science Labs}},
    howpublished = {\url{http://mozillascience.org/}},
    note = {Accessed 31 August 2014},
}

@Misc{openscienceframework,
    title = {{Open Science Framework}},
    key = {{Open Science Framework}},
    howpublished = {\url{https://osf.io/}},
    note = {Accessed 31 August 2014},
}

@Misc{runmycode,
    title = {{RunMyCode}},
    key = {{RunMyCode}},
    howpublished = {\url{http://www.runmycode.org/}},
    note = {Accessed 31 August 2014},
}

@Misc{softwarecarpentry,
    title = {{Software Carpentry}},
    key = {{Software Carpentry}},
    howpublished = {\url{http://software-carpentry.org/}},
    note = {Accessed 31 August 2014},
}

@Misc{edinburgh:ipr,
    author = {{The University of Edinburgh}},
    key = {Edinburgh},
    title = {Position Statement on Commercialising University Intellectual Property},
    month = dec,
    year = 2011,
    url =
    {http://www.research-innovation.ed.ac.uk/Portals/0/Documents/University-of-Edinburgh-Position-Statement-on-Intellectual-Property_December2011.pdf}
}

@Misc{manchester:ipr,
    author = {{The University of Manchester}},
    key = {Manchester},
    title = {Intellectual Property Policy},
    month = jan,
    year = 2013,
    url = {http://documents.manchester.ac.uk/display.aspx?DocID=487}
}

@misc{shima2007simulation,
  title={Simulation method, simulation program, and simulator},
  author={Shima, S. and Sugiyama, T. and Kusano, K. and Kawano, A. and Hirose, S.},
  url={https://register.epo.org/application?number=EP07008132},
  year = 2007,
  month = oct,
  day = 24,
  note={European Patent EP1847939},
}

@Manual{spsstor,
   title = {{SPSStoR}: {SPSS} syntax to {R} syntax},
   author = {Brandon LeBeau},
   note = {R package version 0.1},
   url = {https://github.com/lebebr01/SPSStoR}
 }

@Misc{replichiVenue,
    title = {{RepliCHI}: Revisiting {HCI} Findings},
    key = {RepliCHI},
    howpublished = {\url{http://replichi.com/}},
    note = {Accessed 1 September 2014},
}

@article{lubin:replicability,
    author = {Lubin, Ardie},
    doi = {10.1037/h0039746},
    issn = {0003-066X},
    journal = {American Psychologist},
    keywords = {psychology, replicability, reproducibility},
    month = aug,
    number = {8},
    pages = {519--520},
    title = {Replicability as a publication criterion},
    url = {http://dx.doi.org/10.1037/h0039746},
    volume = {12},
    year = {1957}
}


@misc{Nacenta:memorability_data,
    title = {Memorability of pre-designed and user-defined gesture sets: experimental dataset},
    howpublished = {\url{http://research-repository.st-andrews.ac.uk/handle/10023/3486}},
    note = {Accessed 1 September 2014},
    journal = {University of St Andrews Research Repository},
    author = {Nacenta, Miguel A. and Kamber, Yemliha and Qiang, Yizhou and Kristensson, Per Ola}
}

@inproceedings{Wilson:2011,
 author = {Wilson, Max L. and Mackay, Wendy and Chi, Ed and Bernstein, Michael and Russell, Dan and Thimbleby, Harold},
 title = {RepliCHI - CHI Should Be Replicating and Validating Results More: Discuss},
 booktitle = {CHI '11 Extended Abstracts on Human Factors in Computing Systems},
 year = {2011},
 isbn = {978-1-4503-0268-5},
 location = {Vancouver, BC, Canada},
 pages = {463--466},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1979742.1979491},
 doi = {10.1145/1979742.1979491},
 acmid = {1979491},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hci, replication, research, science},
} 

@inproceedings{Mackay:2007,
 author = {Mackay, Wendy E. and Appert, Caroline and Beaudouin-Lafon, Michel and Chapuis, Olivier and Du, Yangzhou and        Fekete, Jean-Daniel and Guiard, Yves},
 title = {Touchstone: Exploratory Design of Experiments},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 year = {2007},
 isbn = {978-1-59593-593-9},
 location = {San Jose, California, USA},
 pages = {1425--1434},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1240624.1240840},
 doi = {10.1145/1240624.1240840},
 acmid = {1240840},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarking, experimental design, interaction techniques, touchstone},
}

@book{hartson:2012,
  title={The UX book: process and guidelines for ensuring a quality user experience},
  author={Hartson, Rex and Pyla, Pardha S},
  year={2012},
  publisher={Elsevier}
}
