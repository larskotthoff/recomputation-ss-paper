
@Misc{gent:recomputation,
  author      =	{Gent, Ian P.},
  title	      =	{The Recomputation Manifesto},
  abstract    =	{Replication of scientific experiments is critical to the advance of science. Unfortunately, the discipline of Computer Science has never treated replication seriously, even though computers are very good at doing the same thing over and over again. Not only are experiments rarely replicated, they are rarely even replicable in a meaningful way. Scientists are being encouraged to make their source code available, but this is only a small step. Even in the happy event that source code can be built and run successfully, running code is a long way away from being able to replicate the experiment that code was used for. I propose that the discipline of Computer Science must embrace replication of experiments as standard practice. I propose that the only credible technique to make experiments truly replicable is to provide copies of virtual machines in which the experiments are validated to run. I propose that tools and repositories should be made available to make this
		happen. I propose to be one of those who makes it happen.},
  archiveprefix={arXiv},
  day	      =	{12},
  eprint      =	{1304.3674},
  keywords    =	{recomputation, reproducibility, research, virtualisation},
  month	      =	apr,
  url	      =	{http://arxiv.org/abs/1304.3674},
  year	      =	{2013}
}

@article{ciaran,
  Author = {McCreesh, Ciaran and Prosser, Patrick},
	Title = {The Shape of the Search Tree for the Maximum Clique Problem, and the Implications for Parallel Branch and Bound},
  Year = {2014},
	url = {http://arxiv.org/abs/1401.5921}}
  
@url{azure,
	Howpublished = {\url{https://azure.microsoft.com/en-us/}},
	Title = {{Microsoft Azure}}}

@inproceedings{chord,
author = {Stoica, Ion and Morris, Robert and Karger, David and Kaashoek, M. Frans and Balakrishnan, Hari},
 title = {Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications},
 booktitle = {Proceedings of the 2001 Conference on Applications, Technologies, Architectures, and Protocols for Computer Communications},
 year = {2001},
 isbn = {1-58113-411-8},
 location = {San Diego, CA, USA},
 pages = {149--160},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/383059.383071},
 doi = {10.1145/383059.383071},
 acmid = {383071},
 publisher = {ACM},
 address = {New York, NY, USA},
} 

@inproceedings{danielpaper,
  Author = {Reijsbergen, Dani\"el and Gilmore, Stephen},
	Title = {Formal punctuality analysis of frequent bus
services using headway data},
booktitle={Proceedings of the Eleventh European Workshop on Performance Engineering (EPEW 2014)},
  Year = {2014},
  month = sep,
	}
	
	
@article{bareford2010nanoflare,
  title={A nanoflare distribution generated by repeated relaxations triggered by kink instability},
  author={Bareford, M and Browning, P and Van der Linden, R},
  doi={10.1051/0004-6361/201014067},
  journal={Astronomy and astrophysics},
  volume={521},
  month=oct,
  year={2010},
  publisher={EDP Sciences}
}

@article{bareford2011flare,
  title={The flare-energy distributions generated by kink-unstable ensembles of zero-net-current coronal loops},
  author={Bareford, M and Browning, P and Van der Linden, R},
  journal={Solar Physics},
  volume={273},
  number={1},
  pages={93--115},
  year={2011},
  publisher={Springer}
}

@article{arabas2013libcloud,
  title={libcloudph{++} 0.1: single-moment bulk, double-moment bulk, and particle-based warm-rain microphysics library in {C++}},
  author={Arabas, Sylwester and Jaruga, Anna and Pawlowska, Hanna and Grabowski, Wojciech W},
  journal={arXiv preprint arXiv:1310.1905},
  year={2013}
}

@inproceedings{hornbaek:replications,
    abstract = {{A replication is an attempt to confirm an earlier study's findings. It is often claimed that research in Human-Computer Interaction (HCI) contains too few replications. To investigate this claim we examined four publication outlets (891 papers) and found 3\% attempting replication of an earlier result. The replications typically confirmed earlier findings, but treated replication as a confirm/not-confirm decision, rarely analyzing effect sizes or comparing in depth to the replicated paper. When asked, most authors agreed that their studies were replications, but rarely planned them as such. Many non-replication studies could have corroborated earlier work if they had analyzed data differently or used minimal effort to collect extra data. We discuss what these results mean to HCI, including how reporting of studies could be improved and how conferences/journals may change author instructions to get more replications.}},
    address = {New York, NY, USA},
    author = {Hornbaek, Kasper and Sander, S{\o}ren S. and Javier Andr\'{e}s Bargas Avila and Simonsen, Jakob G.},
    booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
    citeulike-article-id = {13160301},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=2557004},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/2556288.2557004},
    doi = {10.1145/2556288.2557004},
    isbn = {978-1-4503-2473-1},
    keywords = {hci, repro},
    location = {Toronto, Ontario, Canada},
    pages = {3523--3532},
    posted-at = {2014-05-06 14:18:35},
    priority = {2},
    publisher = {ACM},
    series = {CHI '14},
    title = {{Is Once Enough?: On the Extent and Content of Replications in Human-computer Interaction}},
    url = {http://dx.doi.org/10.1145/2556288.2557004},
    year = {2014}
  }

@article{kotthoff,
     author = {Kotthoff, Lars},
     title = {Reliability of computational experiments on virtualised hardware},
     journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
     volume = {26},
     number = {1},
     pages = {33-49},
     year = {2014},
     doi = {10.1080/0952813X.2013.784812},
     URL = {http://dx.doi.org/10.1080/0952813X.2013.784812 },
     eprint = {http://dx.doi.org/10.1080/0952813X.2013.784812}
}

@article{GMD_editorial_2013,
  title={Editorial: The publication of geoscientific model developments v1.0},
  author={{GMD Executive Editors}},
  journal={Geoscientific Model Development},
  doi = {10.5194/gmd-6-1233-2013},
  volume={6},
  number={4},
  pages={1233--1242},
  year={2013}
}


@InProceedings{drummond:replicability,
  author      =	{Drummond, Chris},
  title	      =	{Replicability is not Reproducibility: Nor is it Good Science},
  booktitle   =	{Proc. of the Evaluation Methods for Machine Learning Workshop at the 26th ICML},
  keywords    =	{machine-learning, reproducibility, reproducible-research},
  location    =	{Montreal, QC, Canada},
  url	      =	{http://cogprints.org/7691/},
  year	      =	{2009}
}

@Article{stodden:practices,
  author      =	{Stodden, Victoria and Miguez, Sheila},
  title	      =	{Best Practices for Computational Science: Software Infrastructure and Environments for Reproducible and Extensible Research},
  day	      =	{09},
  doi	      =	{10.5334/jors.ay},
  issn	      =	{2049-9647},
  journal     =	{Journal of Open Research Software},
  keywords    =	{computational-science, for:lukehutton, reproducibility, reproducible-research},
  month	      =	jul,
  number      =	{1},
  pages	      =	{21+},
  url	      =	{http://dx.doi.org/10.5334/jors.ay},
  volume      =	{2},
  year	      =	{2014}
}

@Article{mesirov:accessible,
  author      =	{Mesirov, Jill P.},
  title	      =	{Accessible Reproducible Research},
  abstract    =	{10.1126/science.1179653},
  day	      =	{22},
  doi	      =	{10.1126/science.1179653},
  issn	      =	{1095-9203},
  journal     =	{Science},
  keywords    =	{reproducibility, reproducible-research},
  month	      =	jan,
  number      =	{5964},
  pages	      =	{415--416},
  pmid	      =	{20093459},
  publisher   =	{American Association for the Advancement of Science},
  url	      =	{http://dx.doi.org/10.1126/science.1179653},
  volume      =	{327},
  year	      =	{2010}
}

@Article{johnson:evidence,
  author      =	{Johnson, Valen E.},
  title	      =	{Revised standards for statistical evidence.},
  abstract    =	{Recent advances in Bayesian hypothesis testing have led to the development of uniformly most powerful Bayesian tests, which represent an objective, default class of Bayesian hypothesis tests that have the same rejection regions as classical significance tests. Based on the correspondence between these two classes of tests, it is possible to equate the size of classical hypothesis tests with evidence thresholds in Bayesian tests, and to equate P values with Bayes factors. An examination of these connections suggest that recent concerns over the lack of reproducibility of scientific studies can be attributed largely to the conduct of significance tests at unjustifiably high levels of significance. To correct this problem, evidence thresholds required for the declaration of a significant finding should be increased to 25-50:1, and to 100-200:1 for the declaration of a highly significant finding. In terms of classical hypothesis tests, these evidence standards mandate the
		conduct of tests at the 0.005 or 0.001 level of significance.},
  day	      =	{26},
  doi	      =	{10.1073/pnas.1313476110},
  issn	      =	{1091-6490},
  journal     =	{Proceedings of the National Academy of Sciences of the United States of America},
  keywords    =	{research, significance, statistics},
  month	      =	nov,
  number      =	{48},
  pages	      =	{19313--19317},
  pmcid	      =	{PMC3845140},
  pmid	      =	{24218581},
  publisher   =	{National Academy of Sciences},
  url	      =	{http://dx.doi.org/10.1073/pnas.1313476110},
  volume      =	{110},
  year	      =	{2013}
}

@Article{davison:reproducibility,
  author      =	{Davison, Andrew},
  title	      =	{Automated capture of experiment context for easier reproducibility in computational research},
  abstract    =	{Reproducibility is part of the definition of the scientific method. In computational science, reproducibility has practical benefits in allowing code reuse and hence enabling more rapid progress and the ability to develop more complex models and analysis methods. However, it is widely recognized that published research in diverse scientific domains that rely on numerical computation is too infrequently reproducible, with some commentators speaking of a ``credibility crisis''. In this article, I examine the reasons why reproducibility is difficult to achieve, argue that for computational research to become consistently and reliably reproducible requires that reproducibility become \emph{easy} to achieve, as part of day-to-day research and not just for that subset of research that is published, and suggest a combination of best practices and automated tools that can make reproducible research easier.},
  address     =	{Los Alamitos, CA, USA},
  doi	      =	{10.1109/mcse.2012.41},
  issn	      =	{1521-9615},
  journal     =	{Computing in Science and Engineering},
  keywords    =	{context, experiments, for:lukehutton, reproducibility, research, workflow},
  month       = jul,
  number      =	{4},
  pages       = {48--56},
  publisher   =	{IEEE Computer Society},
  url	      =	{http://dx.doi.org/10.1109/mcse.2012.41},
  volume      =	{14},
  year	      =	{2012}
}

@Misc{hutton:reproducibility,
    author = {Luke Hutton and Tristan Henderson},
    title = {Towards reproducibility in online social network 
        research},
    note = {In submission to this same special issue},
    month = aug,
    year = 2014,
}

@Article{donoho:reproducible,
  author      =	{Donoho, David L. and Maleki, Arian and Rahman, Inam U. and Shahram, Morteza and Stodden, Victoria},
  title	      =	{Reproducible Research in Computational Harmonic Analysis},
  abstract    =	{Scientific computation is emerging as absolutely central to the scientific method. Unfortunately, it's error-prone and currently immatureatraditional scientific publication is incapable of finding and rooting out errors in scientific computationawhich must be recognized as a crisis. An important recent development and a necessary response to the crisis is reproducible computational research in which researchers publish the article along with the full computational environment that produces the results. The authors have practiced reproducible computational research for 15 years and have integrated it with their scientific research and with doctoral and postdoctoral education. In this article, they review their approach and how it has evolved over time, discussing the arguments for and against working reproducibly.},
  address     =	{Los Alamitos, CA, USA},
  doi	      =	{10.1109/mcse.2009.15},
  issn	      =	{1521-9615},
  journal     =	{Computing in Science \& Engineering},
  keywords    =	{computational-science, reproducibility},
  month	      =	jan,
  number      =	{1},
  pages	      =	{8--18},
  publisher   =	{IEEE Computer Society},
  url	      =	{http://dx.doi.org/10.1109/mcse.2009.15},
  volume      =	{11},
  year	      =	{2009}
}

@Article{peng:reproducible,
  author      =	{Peng, Roger D.},
  title	      =	{Reproducible Research in Computational Science},
  abstract    =	{Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
  day	      =	{02},
  doi	      =	{10.1126/science.1213847},
  issn	      =	{1095-9203},
  journal     =	{Science},
  keywords    =	{reproducibility, research, science},
  month	      =	dec,
  number      =	{6060},
  pages	      =	{1226--1227},
  pmcid	      =	{PMC3383002},
  pmid	      =	{22144613},
  publisher   =	{American Association for the Advancement of Science},
  url	      =	{http://dx.doi.org/10.1126/science.1213847},
  volume      =	{334},
  year	      =	{2011}
}

@Article{bonnet:repeatability,
  author      =	{Bonnet, Philippe and Manegold, Stefan and Bj{\o}rling, Matias and Cao, Wei and Gonzalez, Javier and Granados, Joel and Hall, Nancy and Idreos, Stratos and Ivanova, Milena and Johnson, Ryan and Koop, David and Kraska, Tim and M\"{u}ller, Ren\'{e} and Olteanu, Dan and Papotti, Paolo and Reilly, Christine and Tsirogiannis, Dimitris and Yu, Cong and Freire, Juliana and Shasha, Dennis},
  title	      =	{Repeatability and Workability Evaluation of {SIGMOD} 2011},
  abstract    =	{SIGMOD has offered, since 2008, to verify the experiments published in the papers accepted at the conference. This year, we have been in charge of reproducing the experiments provided by the authors (repeatability), and exploring changes to experiment parameters (workability). In this paper, we assess the SIGMOD repeatability process in terms of participation, review process and results. While the participation is stable in terms of number of submissions, we find this year a sharp contrast between the high participation from Asian authors and the low participation from American authors. We also find that most experiments are distributed as Linux packages accompanied by instructions on how to setup and run the experiments. We are still far from the vision of executable papers.},
  address     =	{New York, NY, USA},
  doi	      =	{10.1145/2034863.2034873},
  issn	      =	{0163-5808},
  journal     =	{SIGMOD Rec.},
  keywords    =	{reproducibility},
  month	      =	sep,
  number      =	{2},
  pages	      =	{45--48},
  publisher   =	{ACM},
  url	      =	{http://dx.doi.org/10.1145/2034863.2034873},
  volume      =	{40},
  year	      =	{2011}
}

@Article{ioannidis:repeatability,
  author      =	{Ioannidis, John P. A. and Allison, David B. and Ball, Catherine A. and Coulibaly, Issa and Cui, Xiangqin and Culhane, Aedin C. and Falchi, Mario and Furlanello, Cesare and Game, Laurence and Jurman, Giuseppe and Mangion, Jon and Mehta, Tapan and Nitzberg, Michael and Page, Grier P. and Petretto, Enrico and van Noort, Vera},
  title	      =	{Repeatability of published microarray gene expression analyses},
  abstract    =	{Given the complexity of microarray-based gene expression studies, guidelines encourage transparent design and public data availability. Several journals require public data deposition and several public databases exist. However, not all data are publicly available, and even when available, it is unknown whether the published results are reproducible by independent scientists. Here we evaluated the replication of data analyses in 18 articles on microarray-based gene expression profiling published in Nature Genetics in 2005a2006. One table or figure from each article was independently evaluated by two teams of analysts. We reproduced two analyses in principle and six partially or with some discrepancies; ten could not be reproduced. The main reason for failure to reproduce was data unavailability, and discrepancies were mostly due to incomplete data annotation or specification of data processing and analysis. Repeatability of published microarray studies is apparently
		limited. More strict publication rules enforcing public data availability and explicit description of data processing and analysis should be considered.},
  day	      =	{28},
  doi	      =	{10.1038/ng.295},
  issn	      =	{1061-4036},
  journal     =	{Nat Genet},
  keywords    =	{microarray, reproducibility},
  month	      =	feb,
  number      =	{2},
  pages	      =	{149--155},
  pmid	      =	{19174838},
  publisher   =	{Nature Publishing Group},
  url	      =	{http://dx.doi.org/10.1038/ng.295},
  volume      =	{41},
  year	      =	{2009}
}


@Article{howe:reproducible,
  author      =	{Howe, Bill},
  title	      =	{Virtual Appliances, Cloud Computing, and Reproducible Research},
  abstract    =	{As science becomes increasingly computational, reproducibility has become increasingly difficult, perhaps surprisingly. In many contexts, virtualization and cloud computing can mitigate the issues involved without significant overhead to the researcher, enabling the next generation of rigorous and reproducible computational science.},
  doi	      =	{10.1109/mcse.2012.62},
  issn	      =	{1521-9615},
  journal     =	{Computing in Science \& Engineering},
  keywords    =	{cloud-computing, reproducibility, research},
  month	      =	jul,
  number      =	{4},
  pages	      =	{36--41},
  url	      =	{http://dx.doi.org/10.1109/mcse.2012.62},
  volume      =	{14},
  year	      =	{2012}
}

@Misc{pineda-krch:sweave,
    author = {Pineda-Krch, Mario},
    title = {The Joy of {Sweave}: A Beginner's Guide to Reproducible
        Research with {Sweave}},
    year = 2011,
    month = jan,
    day = 17,
    url =
    {http://www.mathstat.ualberta.ca/~mlewis/links/the_joy_of_sweave_v1.pdf},
}

@Article{baker:verify,
  author      =	{Baker, Monya},
  title	      =	{Independent labs to verify high-profile papers},
  day	      =	{14},
  doi	      =	{10.1038/nature.2012.11176},
  issn	      =	{1476-4687},
  journal     =	{Nature},
  keywords    =	{reproducibility, research, science, nopdf},
  month	      =	aug,
  url	      =	{http://dx.doi.org/10.1038/nature.2012.11176},
  year	      =	{2012}
}

@Misc{github:citable,
    author = {GitHub},
    title = {Making Your Code Citable},
    url = {https://guides.github.com/activities/citable-code/},
    year = 2014,
    month = may,
    note = {Accessed 28 August 2014},
}

@Misc{figshare:citable,
    author = {FigShare},
    title = {All research outputs should be citable},
    url =
    {http://figshare.com/blog/All%20research%20outputs%20should%20be%20citable/32},
    year = 2012,
    month = jun,
    day = 12,
    note = {Accessed 28 August 2014},
}


@Misc{NSF,
    title = {A vision and strategy for software for science, engineering and education},
      url = {http://www.nsf.gov/pubs/2012/nsf12113/nsf12113.pdf},
   author = {NSF},  
     year = {2012},
   publisher = {US National Science Foundation}
  }

@inproceedings{lmucs-papers:Leisch:2002,
      author = {Friedrich Leisch},
        title = {Sweave: Dynamic Generation of Statistical Reports Using
                      Literate Data Analysis},
          booktitle = {Compstat 2002 --- Proceedings in Computational
                      Statistics},
            pages = {575--580},
              year = 2002,
                editor = {Wolfgang H{\"a}rdle and Bernd R{\"o}nz},
                  publisher = {Physica Verlag, Heidelberg},
                    note = {ISBN 3-7908-1517-9},
                      url = {http://www.stat.uni-muenchen.de/~leisch/Sweave}
}

@article{Merali_2010,
     title = {Computational science: Error, why scientific programming does not compute},
    author = {Merali, Zeeya},
   journal = {Nature},
    volume = {467},
    number = {7317},
     pages = {775--777},
      year = {2010},
       doi = {10.1038/467775a}
}
