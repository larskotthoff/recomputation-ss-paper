\documentclass{article}
\usepackage{hyperref}   
\usepackage[backend=bibtex, sorting=none]{biblatex}
\bibliography{references}

\title{Reproducing Parallel and Distributed Experiments}
\author{%%
Masih Hajiarabderkani\thanks{University of St Andrews, St Andrews, Fife, UK}, %
Ciaran McCreesh\thanks{University of Glasgow, Glasgow, Scotland}, %
Ruma R. Paul\thanks{Universit\'{e} catholique de Louvain, Louvain-la-Neuve, Belgium}, %
}

\begin{document}
\SweaveOpts{concordance=TRUE}

<<setup, include=FALSE, cache=FALSE, echo=FALSE>>=
Sys.setenv(TEXINPUTS=getwd(),
           BIBINPUTS=getwd(),
           BSTINPUTS=getwd())
@

\maketitle

.\section*{Abstract}
This study investigates the challenges of reproducing parallel and distributed experiments on virtualized environment, in particular on cloud infrastructure. We explore two different type of experiments to analyze the deviation of run-time and behavior of the system between real and virtual machines: i) A multi-core Parallel system and ii) A Peer-to-Peer system.

.\section*{Experiments}
\label{sec:experiments}
The purpose of this section is to describe the experiments we have used to investigate the viability of reproducing parallel and distributed experiments on virtualized exvironment. We have used two experiments: i) A parallel system and ii) A Peer-to-Peer system.

\subsection*{Experiment 1: A Parallel System}
This experiment is called \emph{Parallel Maximum Clique} which finds out maximum clique in a graph by running a parallel program. The aim is to find out how the run time vary between the run on a real/dedicated hardware (from now on referred as \emph{Real Run Time (RRT)}) and virtual machine (VM) on a cloud (from now on referred as \emph{Virtual Run Time (VRT)}). We have used $4$ core of a node with $16$ Intel cores (2*Intel Xeon E5-2640 2GHz). The node has $64$ Gb RAM and has \emph{Scientific Linux $6$} as the operating system. The algorithm used in this experiment is described in \cite{ciaran}. We have selected $10$ different problem instances (i.e. graphs) to have a valid and reasonable comparative analysis. For each instance, we run the algorithm $50$ times once on the real/dedicated harware to collect the RRTs and on a VM with $4$ cores on Microsoft Azure \cite{azure} to collect the VRTs. We meaaure run time of the algorithm and ignore the input and output times. We compare the relative standard deviation (standard deviation as a proportion of the mean, since processing models varies between real and virtual hardware) of RRTs and VRTs of each problem instance. 

\subsection*{Experiment 2: A Peer-to-Peer System}
We have used Chord (\cite{chord1}, \cite{chord2}) as the Peer-to-Peer (P2P) system to run our experiments. To satisfy the pupose of this report, we run each experiment once on real hardware and on the Virtual Machines (VMs) on Microsoft Azure \cite{azure}. We have created a network of 12 nodes, each running on a seperate machine, which is of $4$ cores ($4$*Intel Xeon 2GHz) with $8$ GBs RAM and Linux $2.6$ as the operating system. To have a valid comparison we have created $12$ VMs on Microsoft Azure, each VM runs on single core. 

A \emph{Workload} model is a temporal sequence of lookups with exponentially distributed intervals. The studied workload models are: i) light (mean interval of 10 seconds), and ii) heavy (mean inverval of $1$ second). A \emph{Churn} model is a temporal sequence of node arrivals and departures from the network. The session lengths and down times of nodes are exponentially distributed. The studied churn models are: i) none (nodes stay alive throughout the experiment), ii) light (mean session length and downtime is $10$ minutes) and iii) heavy (mean session length and downtime is $1$ minute). We run each experiment for $1$ hour and repeat each experiment $5$ times. 


.\section*{Result and Analysis}
This section presents the two sets of result of each experiments as described above and performs the comparison in terms of the objective of this report. 

\subsection*{A Parallel System}
We present the result in tabular format. Each row of the table is the standard deviation of RRTs and VRTs of a problem instance. Table~\ref{run_time} shows the result, where the first column is the name of the problem instance i.e. graph that is used for the experiment.

<<echo=FALSE,results=tex>>=
library(xtable)

data.folder = paste(getwd(),"clique_experiments",sep = "/", collapse = NULL)
data_matrix1 = read.csv(file.path(data.folder,"real.csv"), sep=",", row.names=1)
data_matrix2 = read.csv(file.path(data.folder,"cloud.csv"), sep=",", row.names=1)

data_matrix1<-as.matrix(data_matrix1)
data_matrix2<-as.matrix(data_matrix2)

f <- function(x) c(r_std_dev = (sd(x)/mean(x))*100)

result_matrix1 = apply(data_matrix1, 2, f)
result_matrix2 = apply(data_matrix2, 2, f)
 
run_time <- data.frame(real=result_matrix1,
                                vm=result_matrix2)

xtable(run_time,
       caption = "Relative Standard Deviations of Run Time (in percentage)",
       label = "run_time",
       align = "c|rr",
       digits = c(0,10,10))
@

In both real and virtual hardwares the relative standard deviation is very small (below $1\%$ in every case). We did not encounter any abnormalities in case of virtual environment.  

\subsection*{A Peer-to-Peer System}




.\section*{Conclusion}
\printbibliography


\end{document}
